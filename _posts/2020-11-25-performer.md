---
layout: post
title: "Looking at the Performer from a Hopfield point of view"
description: The recent paper Rethinking Attention with Performers constructs a new efficient attention mechanism in an elegant way. It strongly reduces the computational cost for long sequences, while keeping the intriguing properties of the original attention mechanism. In doing so, Performers have a complexity only linear in the input length, in contrast to the quadratic complexity of standard transformers. This is a major breakthrough in the strive of improving transformer models.

date: 2020-11-25
tags: Hopfield Attention Transformers
categories:
- Hopfield
- Transformers
- Performers
- Articles

authors:
 - name: Johannes Brandstetter
   url: "https://www.jku.at/institut-fuer-machine-learning/ueber-uns/team/ass-prof-dr-johannes-brandstetter/"
   affiliations:
      name: Institute for Machine Learning, Linz

 - name: Hubert Ramsauer
   url: "https://www.jku.at/en/institute-for-machine-learning/about-us/team/hubert-ramsauer/"
   affiliations:
      name: Institute for Machine Learning, Linz

forward: true
forward_url: https://ml-jku.github.io/blog-post-performer/

pic: /assets/img/performer/homer_bw.png

---
