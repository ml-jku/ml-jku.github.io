@Article{adler2020cross,
  abbr={arXiv},
  author        = {Adler, Thomas and Brandstetter, Johannes and Widrich, Michael and Mayr, Andreas and Kreil, David and Kopp, Michael and Klambauer, G{\"u}nter and Hochreiter, Sepp},
  title         = {Cross-Domain Few-Shot Learning by Representation Fusion},
  journal       = {arXiv preprint arXiv:2010.06498},
  arxiv         = {2010.06498},
  year          = {2020},
  __markedentry = {[gk:6]},
  selected={true}
}


@online{heuselGANsTrainedTwo2017,
  abbr={Neurips},
  title = {{{GANs Trained}} by a {{Two Time}}-{{Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  date = {2017-06-26},
  year = {2017},
  pdf = {http://arxiv.org/pdf/1706.08500.pdf},
  urldate = {2018-05-08},
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\textbackslash 'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  arxiv = {1706.08500},
  keywords = {Computer Science - Learning,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat},
  selected={true}
}

@online{ramsauerHopfieldNetworksAll2020,
  abbr={arXiv},
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
  date = {2020-07-16},
  pdf = {http://arxiv.org/pdf/2008.02217.pdf},
  urldate = {2020-08-10},
  abstract = {We show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called "Hopfield", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: https://github.com/ml-jku/hopfield-layers},
  arxiv = {2008.02217},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat},
  version = {1},
  selected={true}
}

@Article{widrich2020modern,
  abbr={NeurIPS},
  author        = {Widrich, Michael and Sch{\"a}fl, Bernhard and Ramsauer, Hubert and Pavlovi{\'c}, Milena and Gruber, Lukas and Holzleitner, Markus and Brandstetter, Johannes and Sandve, Geir Kjetil and Greiff, Victor and Hochreiter, Sepp and others},
  title         = {Modern Hopfield networks and attention for immune repertoire classification},
  journal       = {arXiv preprint arXiv:2007.13505},
  arxiv         = {2007.13505},
  year          = {2020},
  __markedentry = {[gk:6]},
  selected={true}
}

@Article{patil2020alignrudder,
  abbr          = {arXiv},
  title         = {Align-RUDDER: Learning From Few Demonstrations by Reward Redistribution},
  author        = {Vihang P. Patil and Markus Hofmarcher and Marius-Constantin Dinu and Matthias Dorfer and Patrick M. Blies and Johannes Brandstetter and Jose A. Arjona-Medina and Sepp Hochreiter},
  year          = {2020},
  arxiv         = {2009.14108},
  abstract      = {Reinforcement Learning algorithms require a large number of samples to solve complex tasks with sparse and delayed rewards. Complex tasks can often be hierarchically decomposed into sub-tasks. A step in the Q-function can be associated with solving a sub-task, where the expectation of the return increases. RUDDER has been introduced to identify these steps and then redistribute reward to them, thus immediately giving reward if sub-tasks are solved. Since the problem of delayed rewards is mitigated, learning is considerably sped up. However, for complex tasks, current exploration strategies as deployed in RUDDER struggle with discovering episodes with high rewards. Therefore, we assume that episodes with high rewards are given as demonstrations and do not have to be discovered by exploration. Typically the number of demonstrations is small and RUDDER's LSTM model as a deep learning method does not learn well. Hence, we introduce Align-RUDDER, which is RUDDER with two major modifications. First, Align-RUDDER assumes that episodes with high rewards are given as demonstrations, replacing RUDDER's safe exploration and lessons replay buffer. Second, we replace RUDDER's LSTM model by a profile model that is obtained from multiple sequence alignment of demonstrations. Profile models can be constructed from as few as two demonstrations as known from bioinformatics. Align-RUDDER inherits the concept of reward redistribution, which considerably reduces the delay of rewards, thus speeding up learning. Align-RUDDER outperforms competitors on complex artificial tasks with delayed reward and few demonstrations. On the MineCraft ObtainDiamond task, Align-RUDDER is able to mine a diamond, though not frequently.},
  journal       = {arXiv preprint arXiv:2009.14108},
  pdf           = {https://arxiv.org/pdf/2009.14108.pdf},
  primaryClass  = {cs.LG},
  selected      = {true}
}


@InProceedings{klotz2020learning,
  author        = {Klotz, Daniel and Kratzert, Frederik and Sampson, Alden K and Klambauer, G{\"u}nter and Hochreiter, Sepp and Nearing, Grey},
  title         = {Learning from mistakes: Online updating for deep learning models.},
  booktitle     = {EGU General Assembly Conference Abstracts},
  year          = {2020},
  pages         = {8853},
  __markedentry = {[gk:6]},
  selected={true}
}

@Article{renz2020failure,
  author        = {Renz, Philipp and Van Rompaey, Dries and Wegner, J{\"o}rg Kurt and Hochreiter, Sepp and Klambauer, G{\"u}nter},
  title         = {On failure modes of molecule generators and optimizers},
  year          = {2020},
  __markedentry = {[gk:6]},
  publisher     = {ChemRxiv},
  selected={true}
}

@Article{mayr2020lsc,
  author        = {Mayr, Andreas and Klambauer, G{\"u}nter and Unterthiner, Thomas and Hochreiter, Sepp},
  title         = {The LSC Benchmark Dataset: Technical Appendix and Partial Reanalysis},
  year          = {2020},
  __markedentry = {[gk:6]},
  selected={true}
}

@online{hofmarcherLargescaleLigandbasedVirtual2020,
  abbr ={arXiv},
  title = {Large-Scale Ligand-Based Virtual Screening for {{SARS}}-{{CoV}}-2 Inhibitors Using Deep Neural Networks},
  author = {Hofmarcher, Markus and Mayr, Andreas and Rumetshofer, Elisabeth and Ruch, Peter and Renz, Philipp and Schimunek, Johannes and Seidl, Philipp and Vall, Andreu and Widrich, Michael and Hochreiter, Sepp and Klambauer, Günter},
  date = {2020-08-17},
  year = {2020},
  pdf = {http://arxiv.org/pdf/2004.00979.pdf},
  urldate = {2020-12-16},
  abstract = {Due to the current severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pandemic, there is an urgent need for novel therapies and drugs. We conducted a large-scale virtual screening for small molecules that are potential CoV-2 inhibitors. To this end, we utilized "ChemAI", a deep neural network trained on more than 220M data points across 3.6M molecules from three public drug-discovery databases. With ChemAI, we screened and ranked one billion molecules from the ZINC database for favourable effects against CoV-2. We then reduced the result to the 30,000 top-ranked compounds, which are readily accessible and purchasable via the ZINC database. Additionally, we screened the DrugBank using ChemAI to allow for drug repurposing, which would be a fast way towards a therapy. We provide these top-ranked compounds of ZINC and DrugBank as a library for further screening with bioassays at https://github.com/ml-jku/sars-cov-inhibitors-chemai.},
  archivePrefix = {arXiv},
  arxiv = {2004.00979},
  eprinttype = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  primaryClass = {cs, q-bio, stat},
  selected = {true}
}

@inproceedings{Arjona:19,
  abbr={NeurIPS},
  title = {RUDDER: Return Decomposition for Delayed Rewards},
  author = {J. A. Arjona-Medina and M. Gillhofer and M. Widrich and
          T. Unterthiner and J. Brandstetter and S. Hochreiter},
  booktitle = {Advances in Neural Information Processing Systems 32},
  abstract = {We propose RUDDER, a novel reinforcement learning approach for delayed rewards in finite Markov decision processes (MDPs). In MDPs the Q-values are equal to the expected immediate reward plus the expected future rewards. The latter are related to bias problems in temporal difference (TD) learning and to high variance problems in Monte Carlo (MC) learning. Both problems are even more severe when rewards are delayed. RUDDER aims at making the expected future rewards zero, which simplifies Q-value estimation to computing the mean of the immediate reward. We propose the following two new concepts to push the expected future rewards toward zero. (i) Reward redistribution that leads to return-equivalent decision processes with the same optimal policies and, when optimal, zero expected future rewards. (ii) Return decomposition via contribution analysis which transforms the reinforcement learning task into a regression task at which deep learning excels. On artificial tasks with delayed rewards, RUDDER is significantly faster than MC and exponentially faster than Monte Carlo Tree Search (MCTS), TD(\{\textbackslash lambda\}), and reward shaping approaches. At Atari games, RUDDER on top of a Proximal Policy Optimization (PPO) baseline improves the scores, which is most prominent at games with delayed rewards. Source code is available at \textbackslash url\{https://github.com/ml-jku/rudder\} and demonstration videos at \textbackslash url\{https://goo.gl/EQerZV\}.},
  arxiv = {1806.07857},
  pages = {13566-13577},
  year = {2019},
  selected={true}
}


@article{nearing2019large,
  abbr={AGUFM},
  title={Large-Scale Rainfall-Runoff Modeling using the Long Short-Term Memory Network},
  author={Nearing, GS and Kratzert, F and Klotz, D and Klambauer, G and Hochreiter, S},
  journal={AGUFM},
  volume={2019},
  pages={H53B--05},
  year={2019},
  selected={true}
}


@inproceedings{klambauer2017self,
  abbr={NeurIPS},
  title={Self-Normalizing Neural Networks},
  author={Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  booktitle={Advances in Neural Information Processing Systems 30},
  pages={972--981},
  year={2017},
  selected={true}
}


@Article{klambauer2012cn,
  author    = {Klambauer, G{\"u}nter and Schwarzbauer, Karin and Mayr, Andreas and Clevert, Djork-Arn{\'e} and Mitterecker, Andreas and Bodenhofer, Ulrich and Hochreiter, Sepp},
  title     = {cn. MOPS: mixture of Poissons for discovering copy number variations in next-generation sequencing data with a low false discovery rate},
  journal   = {Nucleic acids research},
  year      = {2012},
  volume    = {40},
  number    = {9},
  pages     = {e69},
  doi       = {10.1093/nar/gks003},
  publisher = {Oxford University Press},
}

@Article{klambauer2013dexus,
  author    = {Klambauer, G{\"u}nter and Unterthiner, Thomas and Hochreiter, Sepp},
  title     = {DEXUS: identifying differential expression in RNA-Seq studies with unknown conditions},
  journal   = {Nucleic acids research},
  year      = {2013},
  volume    = {41},
  number    = {21},
  pages     = {e198},
  doi       = {10.1093/nar/gkt834},
  publisher = {Oxford University Press},
}

@article{clevert2011cn,
  title={cn. FARMS: a latent variable model to detect copy number variations in microarray data with a low false discovery rate},
  author={Clevert, Djork-Arne and Mitterecker, Andreas and Mayr, Andreas and Klambauer, G{\"u}nter and Tuefferd, Marianne and Bondt, An De and Talloen, Willem and G{\"o}hlmann, Hinrich and Hochreiter, Sepp},
  journal={Nucleic acids research},
  volume={39},
  number={12},
  pages={e79--e79},
  year={2011},
  publisher={Oxford University Press}
}

@Article{clevert2013increasing,
  author    = {Clevert, Djork-Arn{\'e} and Mayr, Andreas and Mitterecker, Andreas and Klambauer, G{\"u}nter and Valsesia, Armand and Forner, Karl and Tuefferd, Marianne and Talloen, Willem and Wojcik, J{\'e}r{\^o}me and G{\"o}hlmann, Hinrich and others},
  title     = {Increasing the discovery power of-omics studies},
  journal   = {Systems Biomedicine},
  year      = {2013},
  volume    = {1},
  number    = {2},
  pages     = {84--93},
  doi       = {10.4161/sysb.25774},
  publisher = {Taylor \& Francis},
}

@inproceedings{karlsson2012enabling,
  title={Enabling large-scale bioinformatics data analysis with cloud computing},
  author={Karlsson, Johan and Torreno, Oscar and Ramet, Daniel and Klambauer, G{\"u}nter and Cano, Miriam and Trelles, Oswaldo},
  booktitle={2012 IEEE 10th International Symposium on Parallel and Distributed Processing with Applications},
  pages={640--645},
  year={2012},
  organization={IEEE}
}

@article{klambauer2015using,
  title={Using transcriptomics to guide lead optimization in drug discovery projects},
  author={Klambauer, G{\"u}nter and Verbist, Bie and Vervoort, Liesbet and Talloen, Willem and TheQSTARConsortium and Shkedy, Ziv and Thas, Olivier and Bender, Andreas and G{\"o}hlmann, Hinrich W.H. and Hochreiter, Sepp},
  journal={Drug Discovery Today},
  volume={20},
  number={5},
  pages={505--513},
  year={2015},
  publisher={Elsevier}
}

@article{clevert2012exploiting,
  title={Exploiting the Japanese toxicogenomics project for predictive modelling of drug toxicity},
  author={Clevert, Djork-Arn{\'e} and Heusel, Martin and Mitterecker, Andreas and Talloen, Willem and G{\"o}hlmann, HWH and Wegner, J{\"o}rg and Mayr, Andreas and Klambauer, G{\"u}nter and Hochreiter, Sepp},
  journal={CAMDA},
  volume={2012},
  pages={26--9},
  year={2012}
}

@inproceedings{bender2013computational,
  title={Computational Methods Aiding Early-Stage Drug Design (Dagstuhl Seminar 13212)},
  author={Bender, Andreas and G{\"o}hlmann, Hinrich and Hochreiter, Sepp and Shkedy, Ziv},
  booktitle={Dagstuhl Reports},
  volume={3},
  number={5},
  year={2013},
  organization={Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik}
}

@Article{schwarzbauer2010identifying,
  author    = {Schwarzbauer, Karin and Klambauer, G{\"u}nter and Mayr, Andreas and Hochreiter, Sepp},
  title     = {Identifying Copy Number Variations based on Next Generation Sequencing Data by a Mixture of Poisson Model},
  journal   = {Nature Proceedings},
  year      = {2010},
  pages     = {1--1},
  doi       = {10.1038/npre.2010.4716.1},
  publisher = {Nature Publishing Group},
}

@Manual{mahr2012package,
  title  = {Package ‘Rchemcpp’},
  author = {Mahr, Michael and Klambauer, Guenter},
  year   = {2012},
}

@Manual{klambauer2013package,
  title  = {Package ‘dexus’},
  author = {Klambauer, Guenter and biocViews Sequencing, RNASeq and GeneExpression, DifferentialExpression},
  year   = {2013},
}

@Manual{Klambauer2013,
  title  = {Package ‘cn. mops’},
  author = {Klambauer, Guenter},
  year   = {2013},
}

@article{klambauer2015rchemcpp,
  title={Rchemcpp: a web service for structural analoging in ChEMBL, Drugbank and the Connectivity Map},
  author={Klambauer, G{\"u}nter and Wischenbart, Martin and Mahr, Michael and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  journal={Bioinformatics},
  volume={31},
  number={20},
  pages={3392--3394},
  year={2015},
  publisher={Oxford University Press}
}

@Article{klambauer2010normalization,
  author    = {Klambauer, G{\"u}nter and Schwarzbauer, Karin and Mayr, Andreas and Hochreiter, Sepp},
  title     = {A normalization technique for next generation sequencing experiments},
  journal   = {Nature Precedings},
  year      = {2010},
  pages     = {1--1},
  doi       = {10.1038/npre.2010.4710.1},
  publisher = {Nature Publishing Group},
}

@inproceedings{unterthiner2014aiding,
  title={Aiding Drug Design with Deep Neural Networks},
  author={Unterthiner, Thomas and Mayr, Andreas and Klambauer, G{\"u}nter and Steijaert, Marvin and Wegner, J{\"o}rg K and Ceulemans, Hugo and Hochreiter, Sepp},
  booktitle={Neural Information Processing Systems Foundation (NIPS 2014)},
  year={2014}
}

@inproceedings{unterthiner2014deep,
  title={Deep learning as an opportunity in virtual screening},
  author={Unterthiner, T. and Mayr, A. and Klambauer, G. and Steijaert, M. and Wegner, J. K. and Ceulemans, H. and Hochreiter, S.},
  booktitle={Deep Learning and Representation Learning Workshop, NIPS 2014},
  year={2014}
}

@Article{eduati2015prediction,
  author  = {Eduati, Federica and Mangravite, Lara M and Wang, Tao and Tang, Hao and Hochreiter, Sepp and Klambauer, G{\"u}nter and Mayr, Andreas and Rusyn, Ivan and Wright, Fred A and Stolovitzky, Gustavo and others},
  title   = {Prediction of human population responses to toxic compounds by a collaborative competition},
  journal = {Nature biotechnology},
  year    = {2015},
  doi     = {10.1038/nbt.3299},
}

@Article{mayr2016deeptox,
  author    = {Mayr, Andreas and Klambauer, G{\"u}nter and Unterthiner, Thomas and Hochreiter, Sepp},
  title     = {DeepTox: toxicity prediction using deep learning},
  journal   = {Frontiers in Environmental Science},
  year      = {2016},
  volume    = {3},
  pages     = {80},
  doi       = {10.3389/fenvs.2015.00080},
  publisher = {Frontiers},
}

@article{unterthiner2015toxicity,
  title={Toxicity prediction using deep learning},
  author={Unterthiner, Thomas and Mayr, Andreas and Klambauer, G{\"u}nter and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:1503.01445},
  year={2015}
}

@Article{wittwehr2017adverse,
  author    = {Wittwehr, Clemens and Aladjov, Hristo and Ankley, Gerald and Byrne, Hugh J and de Knecht, Joop and Heinzle, Elmar and Klambauer, G{\"u}nter and Landesmann, Brigitte and Luijten, Mirjam and MacKay, Cameron and others},
  title     = {How adverse outcome pathways can aid the development and use of computational prediction models for regulatory toxicology},
  journal   = {Toxicological Sciences},
  year      = {2017},
  volume    = {155},
  number    = {2},
  pages     = {326--336},
  doi       = {10.1093/toxsci/kfw207},
  publisher = {Oxford University Press},
}

@Article{sharma2016anti,
  author    = {Sharma, N and Pulito, C and Klambauer, G and Mattoli, L and Strano, S and Blandino, G and Lucci, J and Bender, A},
  title     = {Anti-mesothelioma mechanism of action studies of a complex Cynara scolymus fraction using in silico target prediction and gene expression profiling},
  journal   = {Planta Medica},
  year      = {2016},
  volume    = {82},
  number    = {S 01},
  pages     = {P73},
  doi       = {10.1055/s-0036-1596242},
  publisher = {Georg Thieme Verlag KG},
}

@article{simm2018repurposed,
  title={Repurposed high-throughput image assays enables biological activity prediction for drug discovery},
  author={Simm, Jaak and Klambauer, Guenter and Arany, Adam and Steijaert, Marvin and Wegner, Joerg Kurt and Gustin, Emmanuel and Chupakhin, Vladimir and Chong, Yolanda T and Vialard, Jorge and Buijnsters, Peter and others},
  journal={Cell Chemical Biology},
  pages={108399},
  year={2018},
  publisher={Cold Spring Harbor Laboratory}
}

@Article{povysil2017panelcn,
  author  = {Povysil, Gundula and Tzika, Antigoni and Vogt, Julia and Haunschmid, Verena and Messiaen, Ludwine and Zschocke, Johannes and Klambauer, G{\"u}nter and Hochreiter, Sepp and Wimmer, Katharina},
  title   = {panelcn. MOPS: Copy-number detection in targeted NGS panel data for clinical diagnostics},
  journal = {Human mutation},
  year    = {2017},
  volume  = {38},
  number  = {7},
  pages   = {889--897},
  doi     = {10.1002/humu.23237},
}

@inproceedings{martin2015elu,
  abbr={ICCV},
  title={ELU-Networks - Fast and Accurate CNN Learning on ImageNet},
  author={Martin Heusel, Djork-Arn{\'e} Clevert, G{\"u}nter Klambauer, Andreas Mayr, Karin Schwarzbauer, Thomas Unterthiner, Sepp Hochreiter},
  booktitle={International Conference on Computer Vision (ICCV 2015)},
  year={2015}
}

@online{unterthinerCoulombGANsProvably2018,
  abbr={ICLR},
  title = {Coulomb {{GANs}}: {{Provably Optimal Nash Equilibria}} via {{Potential Fields}}},
  shorttitle = {Coulomb {{GANs}}},
  author = {Unterthiner, Thomas and Nessler, Bernhard and Seward, Calvin and Klambauer, Günter and Heusel, Martin and Ramsauer, Hubert and Hochreiter, Sepp},
  date = {2018-01-30},
  year = {2018},
  url = {http://arxiv.org/abs/1708.08819},
  urldate = {2020-12-16},
  abstract = {Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on a variety of image datasets. On LSUN and celebA, Coulomb GANs set a new state of the art and produce a previously unseen variety of different samples.},
  arxiv = {1708.08819},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat},
  selected={true}
}

@online{sewardFirstOrderGenerative2018,
  abbr={ICML},
  title = {First {{Order Generative Adversarial Networks}}},
  author = {Seward, Calvin and Unterthiner, Thomas and Bergmann, Urs and Jetchev, Nikolay and Hochreiter, Sepp},
  date = {2018-06-07},
  year = {2018},
  pdf = {http://arxiv.org/pdf/1802.04591.pdf},
  urldate = {2020-12-16},
  abstract = {GANs excel at learning high dimensional distributions, but they can update generator parameters in directions that do not correspond to the steepest descent direction of the objective. Prominent examples of problematic update directions include those used in both Goodfellow's original GAN and the WGAN-GP. To formally describe an optimal update direction, we introduce a theoretical framework which allows the derivation of requirements on both the divergence and corresponding method for determining an update direction, with these requirements guaranteeing unbiased mini-batch updates in the direction of steepest descent. We propose a novel divergence which approximates the Wasserstein distance while regularizing the critic's first order information. Together with an accompanying update direction, this divergence fulfills the requirements for unbiased steepest descent updates. We verify our method, the First Order GAN, with image generation on CelebA, LSUN and CIFAR-10 and set a new state of the art on the One Billion Word language generation task. Code to reproduce experiments is available.},
  archivePrefix = {arXiv},
  arxiv = {1802.04591},
  eprinttype = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@Article{eigner2017unfolded,
  author    = {Eigner, Karin and Filik, Y{\"u}ksel and Mark, Florian and Sch{\"u}tz, Birgit and Klambauer, G{\"u}nter and Moriggl, Richard and Hengstschl{\"a}ger, Markus and Stangl, Herbert and Mikula, Mario and R{\"o}hrl, Clemens},
  title     = {The unfolded protein response impacts melanoma progression by enhancing FGF expression and can be antagonized by a chemical chaperone},
  journal   = {Scientific reports},
  year      = {2017},
  volume    = {7},
  number    = {1},
  pages     = {1--12},
  doi       = {10.1038/s41598-017-17888-9},
  publisher = {Nature Publishing Group},
}

@InProceedings{eigner2016impact,
  author       = {Eigner, K and Filik, Y and Klambauer, G and Swoboda, A and Moriggl, R and Stangl, H and Mikula, M and Roehrl, C},
  title        = {Impact of endoplasmic reticulum stress on melanoma malignancy},
  booktitle    = {Melanoma Research},
  year         = {2016},
  volume       = {26},
  pages        = {E37--E37},
  organization = {LIPPINCOTT WILLIAMS \& WILKINS TWO COMMERCE SQ, 2001 MARKET ST, PHILADELPHIA~…},
}

@Article{preuer2018deepsynergy,
  author    = {Preuer, Kristina and Lewis, Richard PI and Hochreiter, Sepp and Bender, Andreas and Bulusu, Krishna C and Klambauer, G{\"u}nter},
  title     = {DeepSynergy: predicting anti-cancer drug synergy with Deep Learning},
  journal   = {Bioinformatics},
  year      = {2018},
  volume    = {34},
  number    = {9},
  pages     = {1538--1546},
  doi       = {10.1093/bioinformatics/btx806},
  publisher = {Oxford University Press},
}


@Article{mayr2018large,
  author    = {Mayr, Andreas and Klambauer, G{\"u}nter and Unterthiner, Thomas and Steijaert, Marvin and Wegner, J{\"o}rg K and Ceulemans, Hugo and Clevert, Djork-Arn{\'e} and Hochreiter, Sepp},
  title     = {Large-scale comparison of machine learning methods for drug target prediction on ChEMBL},
  journal   = {Chemical science},
  year      = {2018},
  volume    = {9},
  number    = {24},
  pages     = {5441--5451},
  doi       = {10.1039/c8sc00148k},
  publisher = {Royal Society of Chemistry},
}

@Article{sturm2018application,
  author    = {Sturm, No{\'e} and Sun, Jiangming and Vandriessche, Yves and Mayr, Andreas and Klambauer, Günter and Carlsson, Lars and Engkvist, Ola and Chen, Hongming},
  title     = {Application of bioactivity profile-based fingerprints for building machine learning models},
  journal   = {Journal of chemical information and modeling},
  year      = {2018},
  volume    = {59},
  number    = {3},
  pages     = {962--972},
  doi       = {10.26434/chemrxiv.6969584.v1},
  publisher = {American Chemical Society},
}

@Article{preuer2018frechet,
  author    = {Preuer, Kristina and Renz, Philipp and Unterthiner, Thomas and Hochreiter, Sepp and Klambauer, Günter},
  title     = {Fr{\'e}chet ChemNet distance: a metric for generative models for molecules in drug discovery},
  journal   = {Journal of chemical information and modeling},
  year      = {2018},
  volume    = {58},
  number    = {9},
  pages     = {1736--1741},
  doi       = {10.1021/acs.jcim.8b00234},
  publisher = {American Chemical Society},
}

@Manual{de2016biclustgui,
  title     = {The biclustGUI Shiny App},
  author    = {De Troyer, Ewoud and Sengupta, Rudradev and Martin Otava, Jitao David and Zhang, Sebastian Kaiser and Culhane, Aedin and Gusenleitner, Daniel and Gestraud, Pierre and Csardi, Gabor and Hochreiter, Sepp and Klambauer, Gunter and others},
  year      = {2016},
  publisher = {CRC Press Taylor \& Francis Group},
}

@inproceedings{rumetshoferHumanlevelProteinLocalization2018,
  abbr={ICLR},
  title = {Human-Level {{Protein Localization}} with {{Convolutional Neural Networks}}},
  author = {Rumetshofer, Elisabeth and Hofmarcher, Markus and Röhrl, Clemens and Hochreiter, Sepp and Klambauer, Günter},
  date = {2018-09-27},
  year = {2018},
  pdf = {https://openreview.net/forum?id=ryl5khRcKm},
  urldate = {2020-12-16},
  abstract = {Localizing a specific protein in a human cell is essential for understanding cellular functions and biological processes of underlying diseases. A promising, low-cost,and time-efficient...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english}
}


@article{kratzert2018internals,
  title={Do internals of neural networks make sense in the context of hydrology?},
  author={Kratzert, Frederik and Herrnegger, Mathew and Klotz, Daniel and Hochreiter, Sepp and Klambauer, G{\"u}nter},
  journal={AGUFM},
  volume={2018},
  pages={H13B--06},
  year={2018}
}

@inproceedings{kratzert2019using,
  title={Using large data sets towards generating a catchment aware hydrological model for global applications.},
  author={Kratzert, Frederik and Klotz, Daniel and Herrnegger, Mathew and Hochreiter, Sepp and Klambauer, G{\"u}nter},
  booktitle={Geophysical Research Abstracts},
  volume={21},
  year={2019}
}

@inproceedings{klotz2019towards,
  title={Towards the quantification of uncertainty for deep learning based rainfall-runoff models.},
  author={Klotz, Daniel and Kratzert, Frederik and Herrnegger, Mathew and Hochreiter, Sepp and Klambauer, G{\"u}nter},
  booktitle={Geophysical Research Abstracts},
  volume={21},
  year={2019}
}

@InCollection{preuer2019interpretable,
  author    = {Preuer, Kristina and Klambauer, G{\"u}nter and Rippmann, Friedrich and Hochreiter, Sepp and Unterthiner, Thomas},
  title     = {Interpretable deep learning in drug discovery},
  booktitle = {Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
  publisher = {Springer, Cham},
  year      = {2019},
  pages     = {331--345},
  doi       = {10.1007/978-3-030-28954-6_18},
}

@Article{hofmarcher2019accurate,
  author    = {Hofmarcher, Markus and Rumetshofer, Elisabeth and Clevert, Djork-Arne and Hochreiter, Sepp and Klambauer, Günter},
  title     = {Accurate prediction of biological assays with high-throughput microscopy images and convolutional networks},
  journal   = {Journal of chemical information and modeling},
  year      = {2019},
  volume    = {59},
  number    = {3},
  pages     = {1163--1171},
  doi       = {10.1021/acs.jcim.8b00670},
  publisher = {American Chemical Society},
}

@InCollection{kratzert2019neuralhydrology,
  author    = {Kratzert, Frederik and Herrnegger, Mathew and Klotz, Daniel and Hochreiter, Sepp and Klambauer, G{\"u}nter},
  title     = {NeuralHydrology--Interpreting LSTMs in Hydrology},
  booktitle = {Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
  publisher = {Springer, Cham},
  year      = {2019},
  pages     = {347--362},
  doi       = {10.1007/978-3-030-28954-6_19},
}

@InProceedings{kimeswenger2019power,
  author       = {Kimeswenger, S and Klambauer, G and Lang, G and Silye, R and Hotzenecker, W},
  title        = {The power of neural networks to detect cutaneous basal cell carcinomas in histological sections},
  booktitle    = {Experimental Dermatology},
  year         = {2019},
  volume       = {28},
  number       = {3},
  pages        = {E38--E38},
  organization = {WILEY 111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
}

@InProceedings{martin2017gans,
  abbr={NeurIPS},
  author    = {Martin Heusel And Hubert Ramsauer And Thomas Unterthiner And Bernhard Nessler And G{\"u}nter Klambauer And Sepp Hochreiter},
  title     = {GANs Trained by a Two Time-Scale Update Rule Converge to a Nash Equilibrium},
  booktitle = {https://arxiv.org/abs/1706.08500v1},
  year      = {2017},
}

@Article{kratzert2019benchmarking,
  author  = {Kratzert, Frederik and Klotz, Daniel and Shalev, Guy and Klambauer, G{\"u}nter and Hochreiter, Sepp and Nearing, Grey},
  title   = {Benchmarking a catchment-aware Long Short-Term Memory Network (LSTM) for large-scale hydrological modeling},
  journal = {arXiv preprint arXiv:1907.08456},
  year    = {2019},
  doi     = {10.5194/hess-2019-368},
}

@Article{kimeswenger2019452,
  author    = {Kimeswenger, S and Klambauer, G and Lang, G and Hofmarcher, M and Tschandl, P and Sinz, C and Petronio, G and Silye, R and Kittler, H and H{\"o}tzenecker, W},
  title     = {Neural networks detect cutaneous basal cell carcinomas in histological sections (452)},
  journal   = {Journal of Investigative Dermatology},
  year      = {2019},
  volume    = {139},
  number    = {9},
  pages     = {S292},
  publisher = {Elsevier},
}

@InCollection{hofmarcher2019visual,
  abbr={Springer},
  author    = {Hofmarcher, Markus and Unterthiner, Thomas and Arjona-Medina, Jos{\'e} and Klambauer, G{\"u}nter and Hochreiter, Sepp and Nessler, Bernhard},
  title     = {Visual scene understanding for autonomous driving using semantic segmentation},
  booktitle = {Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
  publisher = {Springer, Cham},
  year      = {2019},
  pages     = {285--296},
  doi       = {10.1007/978-3-030-28954-6_15},
}

@Article{vondra2019metabolism,
  author    = {Vondra, Sigrid and Kunihs, Victoria and Eberhart, Tanja and Eigner, Karin and Bauer, Raimund and Haslinger, Peter and Haider, Sandra and Windsperger, Karin and Klambauer, G{\"u}nter and Sch{\"u}tz, Birgit and others},
  title     = {Metabolism of cholesterol and progesterone is differentially regulated in primary trophoblastic subtypes and might be disturbed in recurrent miscarriages},
  journal   = {Journal of lipid research},
  year      = {2019},
  volume    = {60},
  number    = {11},
  pages     = {1922--1934},
  doi       = {10.1194/jlr.p093427},
  publisher = {American Society for Biochemistry and Molecular Biology},
}

@inproceedings{kratzert2019large,
  title={Large-Scale Rainfall-Runoff Modeling using the Long Short-Term Memory Network},
  author={Kratzert, Frederik and Klotz, Daniel and Klambauer, G{\"u}nter and Hochreiter, Sepp and Nearing, Grey Stephen},
  booktitle={AGU Fall Meeting 2019},
  year={2019},
  organization={AGU}
}

@inproceedings{povysil2018panelcn,
  title={panelcn. MOPS: CNV detection in targeted NGS panel data for clinical diagnostics},
  author={Povysil, Gundula and Tzika, A and Vogt, J and Haunschmid, V and Messiaen, L and Zschocke, J and Klambauer, G and Hochreiter, S and Wimmer, K},
  booktitle={EUROPEAN JOURNAL OF HUMAN GENETICS},
  volume={26},
  pages={698--698},
  year={2018},
  organization={NATURE PUBLISHING GROUP MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND}
}

@Article{kimeswenger2019detecting,
  author  = {Kimeswenger, Susanne and Rumetshofer, Elisabeth and Hofmarcher, Markus and Tschandl, Philipp and Kittler, Harald and Hochreiter, Sepp and H{\"o}tzenecker, Wolfram and Klambauer, G{\"u}nter},
  title   = {Detecting cutaneous basal cell carcinomas in ultra-high resolution and weakly labelled histopathological images},
  journal = {Workshop on Machine Learning for Health (ML4H at 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.},
  year    = {2019},
}

@Article{kratzert2019towards,
  author  = {Kratzert, Frederik and Klotz, Daniel and Shalev, Guy and Klambauer, G{\"u}nter and Hochreiter, Sepp and Nearing, Grey},
  title   = {Towards learning universal, regional, and local hydrological behaviors via machine learning applied to large-sample datasets.},
  journal = {Hydrology \& Earth System Sciences},
  year    = {2019},
  volume  = {23},
  number  = {12},
  doi     = {10.5194/hess-23-5089-2019},
}

@inproceedings{unterthiner2014multi,
  abbr={NeurIPS},
  title={Multi-task deep networks for drug target prediction},
  author={Unterthiner, Thomas and Mayr, Andreas and Klambauer, G{\"u}nter and Steijaert, Marvin and Wegner, J{\"o}rg K and Ceulemans, Hugo and Hochreiter, Sepp},
  booktitle={Neural Information Processing System},
  volume={2014},
  pages={1--4},
  year={2014},
  organization={NeurIPS}
}

@Article{Preuer2018,
  author  = {Preuer, Kristina and Renz, Philipp and Unterthiner, Thomas and Hochreiter, Sepp and Klambauer, G{\"u}nter},
  title   = {Fr{\'e}chet ChemblNet Distance: A metric for generative models for molecules},
  journal = {arXiv preprint arXiv:1803.09518},
  year    = {2018},
}

@Article{Preuer2018a,
  author    = {Preuer, Kristina and Renz, Philipp and Unterthiner, Thomas and Hochreiter, Sepp and Klambauer, Günter},
  title     = {Fr{\'e}chet ChemNet distance: a metric for generative models for molecules in drug discovery},
  journal   = {Journal of chemical information and modeling},
  year      = {2018},
  volume    = {58},
  number    = {9},
  pages     = {1736--1741},
  doi       = {10.1021/acs.jcim.8b00234},
  publisher = {ACS Publications},
}

@TechReport{mayr2020lsc,
  author      = {Mayr, Andreas and Klambauer, G{\"u}nter and Unterthiner, Thomas and Hochreiter, Sepp},
  title       = {The LSC Benchmark Dataset: Technical Appendix and Partial Reanalysis},
  institution = {LIT AI Lab \& Institute for Machine Learning, Johannes Kepler University Linz},
  year        = {2020},
}


@TechReport{hofmarcher2020large,
  author      = {Hofmarcher, Markus and Mayr, Andreas and Rumetshofer, Elisabeth and Ruch, Peter and Renz, Philipp and Schimunek, Johannes and Seidl, Philipp and Vall, Andreu and Widrich, Michael and Hochreiter, Sepp and others},
  title       = {Large-scale ligand-based virtual screening for SARS-CoV-2 inhibitors using deep neural networks},
  institution = {LIT AI Lab \& Institute for Machine Learning, Johannes Kepler University Linz},
  year        = {2020},
  doi         = {10.2139/ssrn.3561442},
  journal     = {Available at SSRN 3561442},
}

@Article{widrich2020deeprc,
  author    = {Widrich, Michael and Sch{\"a}fl, Bernhard and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and Hochreiter, Sepp and Greiff, Victor and Klambauer, G{\"u}nter},
  title     = {DeepRC: Immune repertoire classification with attention-based deep massive multiple instance learning},
  journal   = {bioRxiv},
  year      = {2020},
  doi       = {10.1101/2020.04.12.038158},
  publisher = {Cold Spring Harbor Laboratory},
}

@article{sturmIndustryscaleApplicationEvaluation2020,
  title = {Industry-Scale Application and Evaluation of Deep Learning for Drug Target Prediction},
  author = {Sturm, Noé and Mayr, Andreas and Le Van, Thanh and Chupakhin, Vladimir and Ceulemans, Hugo and Wegner, Joerg and Golib-Dzib, Jose-Felipe and Jeliazkova, Nina and Vandriessche, Yves and Böhm, Stanislav and Cima, Vojtech and Martinovic, Jan and Greene, Nigel and Vander Aa, Tom and Ashby, Thomas J. and Hochreiter, Sepp and Engkvist, Ola and Klambauer, Günter and Chen, Hongming},
  date = {2020-04-19},
  year = {2020},
  journaltitle = {Journal of Cheminformatics},
  shortjournal = {Journal of Cheminformatics},
  volume = {12},
  pages = {26},
  issn = {1758-2946},
  doi = {10.1186/s13321-020-00428-5},
  url = {https://doi.org/10.1186/s13321-020-00428-5},
  urldate = {2020-12-16},
  abstract = {Artificial intelligence (AI) is undergoing a revolution thanks to the breakthroughs of machine learning algorithms in computer vision, speech recognition, natural language processing and generative modelling. Recent works on publicly available pharmaceutical data showed that AI methods are highly promising for Drug Target prediction. However, the quality of public data might be different than that of industry data due to different labs reporting measurements, different measurement techniques, fewer samples and less diverse and specialized assays. As part of a European funded project (ExCAPE), that brought together expertise from pharmaceutical industry, machine learning, and high-performance computing, we investigated how well machine learning models obtained from public data can be transferred to internal pharmaceutical industry data. Our results show that machine learning models trained on public data can indeed maintain their predictive power to a large degree when applied to industry data. Moreover, we observed that deep learning derived machine learning models outperformed comparable models, which were trained by other machine learning algorithms, when applied to internal pharmaceutical company datasets. To our knowledge, this is the first large-scale study evaluating the potential of machine learning and especially deep learning directly at the level of industry-scale settings and moreover investigating the transferability of publicly learned target prediction models towards industrial bioactivity prediction pipelines.},
  keywords = {Big data,ChEMBL,Cheminformatics,Deep learning,Machine learning,Prospective evaluation,PubChem,QSAR,Retrospective evaluation,Structure-based virtual screening},
  number = {1}
}

@article{renzUncertaintyEstimationMethods2019,
  ids = {renzuncertainty},
  title = {Uncertainty {{Estimation Methods}} to {{Support Decision}}-{{Making}} in {{Early Phases}} of {{Drug Discovery}}},
  author = {Renz, Philipp and Hochreiter, Sepp and Klambauer, Günter},
  year = {2019},
  journaltitle = {NeurIPS-2019 Workshop on  Safety and Robustness in Decision Making},
  abstract = {It takes about a decade to develop a new drug by a process in which a large number of decisions have to be made. Those decisions are critical for the success or failure of a multi-million dollar drug discovery project, which could save many lives or increase life quality. Decisions in early phases of drug discovery, such as the selection of certain series of chemical compounds, are particularly impactful on the success rate. Machine learning models are increasingly used to inform the decision making process by predicting desired effects, undesired effects, such as toxicity, molecular properties, or which wet-lab test to perform next. Thus, accurately quantifying the uncertainties of the models’ outputs is critical, for example, in order to calculate expected utilities, to estimate the risk and the potential gain. In this work, we review, assess and compare recent uncertainty estimation methods with respect to their use in drug discovery projects. We test both, which methods give well calibrated prediction and which ones perform well at misclassification detection. For the latter, we find the entropy of the predictive distribution performs best. Finally, we discuss the problem of defining out-of-distribution samples for prediction tasks on chemical compounds.},
  langid = {english}
}


@InProceedings{kratzert2020towards,
  author        = {Kratzert, Frederik and Klotz, Daniel and Shalev, Guy and Nevo, Sella and Klambauer, G{\"u}nter and Nearing, Grey and Hochreiter, Sepp},
  title         = {Towards deep learning based flood forecasting for ungauged basins},
  booktitle     = {EGU General Assembly Conference Abstracts},
  year          = {2020},
  pages         = {8932},
  __markedentry = {[gk:6]},
}


@Article{mercado2020practical,
  author        = {Mercado, Roc{\'\i}o and Rastemo, Tobias and Lindel{\"o}f, Edvard and Klambauer, G{\"u}nter and Engkvist, Ola and Chen, Hongming and Bjerrum, Esben Jannik},
  title         = {Practical Notes on Building Molecular Graph Generative Models},
  year          = {2020},
  __markedentry = {[gk:6]},
  publisher     = {ChemRxiv},
}

@Article{renz2020failure,
  author        = {Renz, Philipp and Van Rompaey, Dries and Wegner, J{\"o}rg Kurt and Hochreiter, Sepp and Klambauer, G{\"u}nter},
  title         = {On failure modes in molecule generation and optimization},
  journal       = {Drug Discovery Today: Technologies},
  year          = {2020},
  __markedentry = {[gk:6]},
  publisher     = {Elsevier},
}

@string{aps = {American Physical Society,}}



@book{einstein1956investigations,
  title={Investigations on the Theory of the Brownian Movement},
  author={Einstein, Albert},
  year={1956},
  publisher={Courier Corporation,}
}

@article{einstein1950meaning,
  abbr={AJP},
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics,},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers,}
}

@article{PhysRev.47.777,
  abbr={PhysRev},
  title={Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
  author={Einstein, A. and Podolsky, B. and Rosen, N.},
  abstract={In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
  journal={Phys. Rev.,},
  volume={47},
  issue={10},
  pages={777--780},
  numpages={0},
  year={1935},
  month={May},
  publisher=aps,
  doi={10.1103/PhysRev.47.777},
  url={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html={https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf={example_pdf.pdf}
}

@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik,},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}

@article{einstein1905movement,
  abbr={Ann. Phys.},
  title={Un the movement of small particles suspended in statiunary liquids required by the molecular-kinetic theory 0f heat},
  author={Einstein, A.},
  journal={Ann. Phys.,},
  volume={17},
  pages={549--560},
  year={1905}
}

@article{einstein1905electrodynamics,
  title={On the electrodynamics of moving bodies},
  author={Einstein, A.},
  year={1905}
}
