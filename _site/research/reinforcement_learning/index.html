<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Institute for Machine Learning @ JKU | Reinforcement Learning</title>
<meta name="description" content="Research blog of the Institute for Machine Learning @ JKU.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/research/reinforcement_learning/">

<!-- Theming-->




    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>


  <body class="fixed-top-nav ">


    <!-- Header -->

    <!-- <div class="container-fluid mx-0 px-0">
  <div class="logo d-flex justify-content-center align-items-center">
    <img class="jku-logo img-fluid rounded" src="/assets/img/jku_ml_1.svg" width=120px height=40px>
    <img class="ellis-logo img-fluid rounded" src="/assets/img/ellis_linz.jpeg" width=250px height=40px>
  </div>
</div> -->


<header>
    <!-- Nav Bar -->

      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <a class="navbar-brand title font-weight-lighter">
         <!-- <span>Institute</span> for Machine  Learning @ JKU -->
         <img class="rounded" src="/assets/img/jku_ml_1.svg" width=100px height=45px>
         <img class="rounded" src="/assets/img/ellis_linz.jpeg" width=200px height=45px>
        </a>
    <div class="container">
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
                
                    <div class="dropdown">
                      <a class="nav-link" href="/research/">
                        research
                        
                      </a>
                        <div class="dropdown-content">
                          <a href="/research/deep_learning">Deep Learning</a>
                          <a href="/research/ai_healthcare">AI in Health Care</a>
                          <a href="/research/ai_drug">AI in Drug Discovery</a>
                          <a href="/research/reinforcement_learning">Reinforcement Learning</a>
                          <a href="/research/hopfield_networks">Modern Hopfield Networks</a>
                          <a href="/research/ai4earth">AI 4 Earth</a>
                          <a href="/research/ai4drive">AI 4 Driving</a>
                        </div>
                      </div>
                    
                
          </li>
          
          
          
          
          
          
          
          <li class="nav-item ">
                
                <a class="nav-link" href="/publications/">
                  publications
                  
                </a>
                
          </li>
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
                
                <a class="nav-link" href="/talks/">
                  talks
                  
                </a>
                
          </li>
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="https://www.jku.at/en/institute-for-machine-learning/about-us/team/" target="_blank" rel="noopener noreferrer">
                people
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
                
                <a class="nav-link" href="/software/">
                  software
                  
                </a>
                
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="https://www.jku.at/en/institute-for-machine-learning/" target="_blank" rel="noopener noreferrer">
                contact
                
              </a>
          </li>
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    
    <h1 class="post-title">Reinforcement Learning</h1>
    <p class="post-description"></p>
    
  </header>

  <article>
    <div class="about-text">
    <p>Sequential decision making and credit assignment under uncertainty and partial observability is central to developing Intelligent Systems. Reinforcement Learning (RL) provides a general and powerful computational framework for sequential decision making. It involves an agent interacting with the environment to maximize a reward function by selecting actions.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm-5 mt-3 mt-md-0">
        <img class="img-fluid research-img" src="/assets/img/research/reinforcement_learning/rudder_1.png" alt="" title="example image" />
    </div>
<p>

Our research at the Institute of Machine Learning focuses on developing new algorithms and theory required to improve the state of the art in Reinforcement Learning. Credit assignment under delayed reward has been central to our work in recent years. We also actively pursue developing new function approximation methods for scaling Reinforcement Learning to high dimensional problems. Learning to take decisions based on stored data is another area of interest.

We actively apply Reinforcement Learning to various applications including robotics, logistics, natural language processing and others.

<div class="publications">
<h4>recent publications in Reinforcement Learning:</h4>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="holzleitner2020convergence" class="col-sm-8">
    
      <div class="title">Convergence Proof for Actor-Critic Methods Applied to PPO and RUDDER</div>
      <div class="author">
        
          
            
              
                
                  Holzleitner, M.,
                
              
            
          
        
          
            
              
                
                  Gruber, L.,
                
              
            
          
        
          
            
              
                
                  Arjona-Medina, J.,
                
              
            
          
        
          
            
              
                
                  Brandstetter, J.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2020
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="patil2020alignrudder" class="col-sm-8">
    
      <div class="title">Align-RUDDER: Learning From Few Demonstrations by Reward Redistribution</div>
      <div class="author">
        
          
            
              
                
                  Patil, V.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  Dinu, M.,
                
              
            
          
        
          
            
              
                
                  Dorfer, M.,
                
              
            
          
        
          
            
              
                
                  Blies, P.,
                
              
            
          
        
          
            
              
                
                  Brandstetter, J.,
                
              
            
          
        
          
            
              
                
                  Arjona-Medina, J.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2009.14108</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2009.14108" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/align-rudder" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Reinforcement Learning algorithms require a large number of samples to solve complex tasks with sparse and delayed rewards. Complex tasks can often be hierarchically decomposed into sub-tasks. A step in the Q-function can be associated with solving a sub-task, where the expectation of the return increases. RUDDER has been introduced to identify these steps and then redistribute reward to them, thus immediately giving reward if sub-tasks are solved. Since the problem of delayed rewards is mitigated, learning is considerably sped up. However, for complex tasks, current exploration strategies as deployed in RUDDER struggle with discovering episodes with high rewards. Therefore, we assume that episodes with high rewards are given as demonstrations and do not have to be discovered by exploration. Typically the number of demonstrations is small and RUDDER’s LSTM model as a deep learning method does not learn well. Hence, we introduce Align-RUDDER, which is RUDDER with two major modifications. First, Align-RUDDER assumes that episodes with high rewards are given as demonstrations, replacing RUDDER’s safe exploration and lessons replay buffer. Second, we replace RUDDER’s LSTM model by a profile model that is obtained from multiple sequence alignment of demonstrations. Profile models can be constructed from as few as two demonstrations as known from bioinformatics. Align-RUDDER inherits the concept of reward redistribution, which considerably reduces the delay of rewards, thus speeding up learning. Align-RUDDER outperforms competitors on complex artificial tasks with delayed reward and few demonstrations. On the MineCraft ObtainDiamond task, Align-RUDDER is able to mine a diamond, though not frequently.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div>

  <div id="Arjona:19" class="col-sm-8">
    
      <div class="title">RUDDER: Return Decomposition for Delayed Rewards</div>
      <div class="author">
        
          
            
              
                
                  Arjona-Medina, J.,
                
              
            
          
        
          
            
              
                
                  Gillhofer, M.,
                
              
            
          
        
          
            
              
                
                  Widrich, M.,
                
              
            
          
        
          
            
              
                
                  Unterthiner, T.,
                
              
            
          
        
          
            
              
                
                  Brandstetter, J.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://papers.nips.cc/paper/2019/file/16105fb9cc614fc29e1bda00dab60d41-Paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
      <a href="https://ml-jku.github.io/rudder/" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
    
    
      <a href="https://github.com/ml-jku/rudder" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose RUDDER, a novel reinforcement learning approach for delayed rewards in finite Markov decision processes (MDPs). In MDPs the Q-values are equal to the expected immediate reward plus the expected future rewards. The latter are related to bias problems in temporal difference (TD) learning and to high variance problems in Monte Carlo (MC) learning. Both problems are even more severe when rewards are delayed. RUDDER aims at making the expected future rewards zero, which simplifies Q-value estimation to computing the mean of the immediate reward. We propose the following two new concepts to push the expected future rewards toward zero. (i) Reward redistribution that leads to return-equivalent decision processes with the same optimal policies and, when optimal, zero expected future rewards. (ii) Return decomposition via contribution analysis which transforms the reinforcement learning task into a regression task at which deep learning excels. On artificial tasks with delayed rewards, RUDDER is significantly faster than MC and exponentially faster than Monte Carlo Tree Search (MCTS), TD({λ}), and reward shaping approaches. At Atari games, RUDDER on top of a Proximal Policy Optimization (PPO) baseline improves the scores, which is most prominent at games with delayed rewards. Source code is available at \url{https://github.com/ml-jku/rudder} and demonstration videos at \url{https://goo.gl/EQerZV}.</p>
    </div>
    
  </div>
</div>
</li></ol>
</div>
</p></div>

  </div>
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Institute for Machine Learning @ JKU.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
