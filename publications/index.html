<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Institute for Machine Learning @ JKU | publications</title>
<meta name="description" content="Research blog of the Institute for Machine Learning @ JKU.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Theming-->



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-KC0VGMCD6W"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-KC0VGMCD6W');
  </script>


    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
      <a class="navbar-brand title font-weight-lighter" href="ml-jku.github.io/">
       <!-- <span>Institute</span> for Machine  Learning @ JKU -->
       <img class="rounded" src="/assets/img/jku_ml_1.svg" width=100px height=45px>
       <img class="rounded" src="/assets/img/ellis_linz.jpeg" width=200px height=45px>
      </a>
    <div class="container">
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
                
                    <div class="dropdown">
                      <a class="nav-link" href="/research/">
                        research
                        
                      </a>
                        <div class="dropdown-content">
                          <a href="/research/deep_learning">Deep Learning</a>
                          <a href="/research/ai_healthcare">AI in Health Care</a>
                          <a href="/research/ai_drug">AI in Drug Discovery</a>
                          <a href="/research/reinforcement_learning">Reinforcement Learning</a>
                          <a href="/research/hopfield_networks">Modern Hopfield Networks</a>
                          <a href="/research/ai4earth">AI 4 Earth</a>
                          <a href="/research/ai4drive">AI 4 Driving</a>
                        </div>
                      </div>
                    
                
          </li>
          
          
          
          
          
          
          
          <li class="nav-item active">
                
                <a class="nav-link" href="/publications/">
                  publications
                  
                  <span class="sr-only">(current)</span>
                  
                </a>
                
          </li>
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
                
                <a class="nav-link" href="/talks/">
                  talks
                  
                </a>
                
          </li>
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="https://www.jku.at/en/institute-for-machine-learning/about-us/team/" target="_blank" rel="noopener noreferrer">
                people
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
                
                <a class="nav-link" href="/software/">
                  software
                  
                </a>
                
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="https://www.jku.at/en/institute-for-machine-learning/" target="_blank" rel="noopener noreferrer">
                contact
                
              </a>
          </li>
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    
    <h1 class="post-title">publications</h1>
    <p class="post-description">publications in reverse chronological order.</p>
    
  </header>

  <article>
    <div class="about-text">
    <div class="publications">


  <h2 class="year">2023</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">RRL</abbr>
    
  
  </div>

  <div id="schmied2023learning" class="col-sm-8">
    
      <div class="title">Learning to Modulate pre-trained Models in RL</div>
      <div class="author">
        
          
            
              
                
                  Schmied, T.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  Paischer, F.,
                
              
            
          
        
          
            
              
                
                  Pascanu, R.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://openreview.net/forum?id=Us6BtPZGei3" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Reinforcement Learning (RL) has experienced great success in complex games and simulations. However, RL agents are often highly specialized for a particular task, and it is difficult to adapt a trained agent to a new task. In supervised learning, an established paradigm is multi-task pre-training followed by fine-tuning. A similar trend is emerging in RL, where agents are pre-trained on data collections that comprise a multitude of tasks. Despite these developments, it remains an open challenge how to adapt such pre-trained agents to novel tasks while retaining performance on the pre-training tasks. In this regard, we pre-train an agent on a set of tasks from the Meta-World benchmark suite and adapt it to tasks from Continual-World. We conduct a comprehensive comparison of fine-tuning methods originating from supervised learning in our setup. Our findings show that fine-tuning is feasible, but for existing methods, performance on previously learned tasks often deteriorates. Therefore, we propose a novel approach that avoids forgetting by modulating the information flow of the pre-trained model. Our method outperforms existing fine-tuning approaches, and achieves state-of-the-art performance on the Continual-World benchmark. To facilitate future research in this direction, we collect datasets for all Meta-World tasks and make them publicly available.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div>

  <div id="sanokowski2023variational" class="col-sm-8">
    
      <div class="title">Variational annealing on graphs for combinatorial optimization</div>
      <div class="author">
        
          
            
              
                
                  Sanokowski, S.,
                
              
            
          
        
          
            
              
                
                  Berghammer, W.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Lehner, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Advances in Neural Information Processing Systems</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
    
    
      <a href="https://arxiv.org/abs/2311.14156" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/VAG-CO" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Patil:23" class="col-sm-8">
    
      <div class="title">Contrastive Abstraction for Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Patil, V.,
                
              
            
          
        
          
            
              
                
                  Rumetshofer, E.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Generlisation in Planning Workshop, NeuRIPS</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
    
    
      <a href="https://openreview.net/forum?id=oMkUQKfsCU" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div>

  <div id="hoedt2023convex-init" class="col-sm-8">
    
      <div class="title">Principled Weight Initialisation for Input-Convex Neural Networks</div>
      <div class="author">
        
          
            
              
                
                  Hoedt, P.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/9062b7d6e522dadf4f7d85d49b60d81e-Abstract.html" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
      
      <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/9062b7d6e522dadf4f7d85d49b60d81e-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/ml-jku/convex-init" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Input-Convex Neural Networks (ICNNs) are networks that guarantee convexity in their input-output mapping. These networks have been successfully applied for energy-based modelling, optimal transport problems and learning invariances.The convexity of ICNNs is achieved by using non-decreasing convex activation functions and non-negative weights. Because of these peculiarities, previous initialisation strategies, which implicitly assume centred weights, are not effective for ICNNs. By studying signal propagation through layers with non-negative weights, we are able to derive a principled weight initialisation for ICNNs. Concretely, we generalise signal propagation theory by removing the assumption that weights are sampled from a centred distribution. In a set of experiments, we demonstrate that our principled initialisation effectively accelerates learning in ICNNs and leads to better generalisation. Moreover, we find that, in contrast to common belief, ICNNs can be trained without skip-connections when initialised correctly. Finally, we apply ICNNs to a real-world drug discovery task and show that they allow for more effective molecular latent space exploration.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="lehner2023maect" class="col-sm-8">
    
      <div class="title">Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget</div>
      <div class="author">
        
          
            
              
                
                  Lehner, J.,
                
              
            
          
        
          
            
              
                
                  Alkin, B.,
                
              
            
          
        
          
            
              
                
                  Fürst, A.,
                
              
            
          
        
          
            
              
                
                  Rumetshofer, E.,
                
              
            
          
        
          
            
              
                
                  Miklautz, L.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2304.10520" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/MAE-CT" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE), efficiently learn a rich representation of the input. However, for adapting to downstream tasks, they require a sufficient amount of labeled data since their rich features code not only objects but also less relevant image background. In contrast, Instance Discrimination (ID) methods focus on objects. In this work, we study how to combine the efficiency and scalability of MIM with the ability of ID to perform downstream classification in the absence of large amounts of labeled data. To this end, we introduce Masked Autoencoder Contrastive Tuning (MAE-CT), a sequential approach that utilizes the implicit clustering of the Nearest Neighbor Contrastive Learning (NNCLR) objective to induce abstraction in the topmost layers of a pre-trained MAE. MAE-CT tunes the rich features such that they form semantic clusters of objects without using any labels. Notably, MAE-CT does not rely on hand-crafted augmentations and frequently achieves its best performances while using only minimal augmentations (crop &amp; flip). Further, MAE-CT is compute efficient as it requires at most 10% overhead compared to MAE pre-training. Applied to large and huge Vision Transformer (ViT) models, MAE–CT excels over previous self-supervised methods trained on ImageNet in linear probing, k-NN and low-shot classification accuracy as well as in unsupervised clustering accuracy. With ViT-H/16 MAE–CT achieves a new state-of-the-art in linear probing of 82.2%.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="tschoellitsch2023using" class="col-sm-8">
    
      <div class="title">Using emergency department triage for machine learning-based admission and mortality prediction</div>
      <div class="author">
        
          
            
              
                
                  Tschoellitsch, T.,
                
              
            
          
        
          
            
              
                
                  Seidl, P.,
                
              
            
          
        
          
            
              
                
                  Böck, C.,
                
              
            
          
        
          
            
              
                
                  Maletzky, A.,
                
              
            
          
        
          
            
              
                
                  Moser, P.,
                
              
            
          
        
          
            
              
                
                  Thumfart, S.,
                
              
            
          
        
          
            
              
                
                  Giretzlehner, M.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Meier, J.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>European Journal of Emergency Medicine</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="schweighofer2023quam" class="col-sm-8">
    
      <div class="title">Quantification of Uncertainty with Adversarial Models</div>
      <div class="author">
        
          
            
              
                
                  Schweighofer, K.,
                
              
            
          
        
          
            
              
                
                  Aichberger, L.,
                
              
            
          
        
          
            
              
                
                  Ielanski, M.,
                
              
            
          
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2307.03217" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/quam" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Quantifying uncertainty is important for actionable predictions in real-world applications. A crucial part of predictive uncertainty quantification is the estimation of epistemic uncertainty, which is defined as an integral of the product between a divergence function and the posterior. Current methods such as Deep Ensembles or MC dropout underperform at estimating the epistemic uncertainty, since they primarily consider the posterior when sampling models. We suggest Quantification of Uncertainty with Adversarial Models (QUAM) to better estimate the epistemic uncertainty. QUAM identifies regions where the whole product under the integral is large, not just the posterior. Consequently, QUAM has lower approximation error of the epistemic uncertainty compared to previous methods. Models for which the product is large correspond to adversarial models (not adversarial examples!). Adversarial models have both a high posterior as well as a high divergence between their predictions and that of a reference model. Our experiments show that QUAM excels in capturing epistemic uncertainty for deep learning models and outperforms previous methods on challenging tasks in the vision domain.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div>

  <div id="paischer2023semantic" class="col-sm-8">
    
      <div class="title">Semantic HELM: An Interpretable Memory for Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Paischer, F.,
                
              
            
          
        
          
            
              
                
                  Adler, T.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2306.09312" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/helm" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Reinforcement learning agents deployed in the real world often have to cope with partially observable environments. Therefore, most agents employ memory mechanisms to approximate the state of the environment. Recently, there have been impressive success stories in mastering partially observable environments, mostly in the realm of computer games like Dota 2, StarCraft II, or MineCraft. However, existing methods lack interpretability in the sense that it is not comprehensible for humans what the agent stores in its memory. In this regard, we propose a novel memory mechanism that represents past events in human language.
Our method uses CLIP to associate visual inputs with language tokens. Then we feed these tokens to a pretrained language model that serves the agent as memory and provides it with a coherent and human-readable representation of the past. We train our memory mechanism on a set of partially observable environments and find that it excels on tasks that require a memory component, while mostly attaining performance on-par with strong baselines on tasks that do not. On a challenging continuous recognition task, where memorizing the past is crucial, our memory mechanism converges two orders of magnitude faster than prior methods. Since our memory mechanism is human-readable, we can peek at an agent’s memory and check whether crucial pieces of information have been stored. This significantly enhances troubleshooting and paves the way toward more interpretable agents.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="paischer2023sitta" class="col-sm-8">
    
      <div class="title">SITTA: A Semantic Image-Text Alignment for Image Captioning</div>
      <div class="author">
        
          
            
              
                
                  Paischer, F.,
                
              
            
          
        
          
            
              
                
                  Adler, T.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2307.05591" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/semantic-image-text-alignment" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embedding space of the pretrained LM via token correspondences. The latter leverages additional data that consists of image-text pairs to construct the mapping directly from vision to language space. Using our semantic mappings, we unlock image captioning for LMs without access to gradient information. By using different sources of data we achieve strong captioning performance on MS-COCO and Flickr30k datasets. Even in the face of limited data, our method partly exceeds the performance of other zero-shot and even finetuned competitors. Our ablation studies show that even LMs at a scale of merely 250M parameters can generate decent captions employing our semantic mappings. Our approach makes image captioning more accessible for institutions with restricted computational resources.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MIDL</abbr>
    
  
  </div>

  <div id="midl2023" class="col-sm-8">
    
      <div class="title">Learning Retinal Representations from Multi-modal Imaging via Contrastive Pre-training</div>
      <div class="author">
        
          
            
              
                
                  Sükei, E.,
                
              
            
          
        
          
            
              
                
                  Rumetshofer, E.,
                
              
            
          
        
          
            
              
                
                  Schmidinger, N.,
                
              
            
          
        
          
            
              
                
                  Schmidt-Erfurth, U.,
                
              
            
          
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  and Bogunović, H.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Medical Imaging with Deep Learning, short paper track</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://openreview.net/forum?id=newlahoISt1" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Contrastive representation learning techniques trained on large multi-modal datasets, such as CLIP and CLOOB, have demonstrated impressive capabilities of producing highly transferable representations for different downstream tasks. In the field of ophthalmology, large multi-modal datasets are conveniently accessible as retinal imaging scanners acquire both 2D fundus images and 3D optical coherence tomography to evaluate the disease. Motivated by this, we propose a CLIP/CLOOB objective-based model to learn joint representations of the two retinal imaging modalities. We evaluate our modelś capability to accurately retrieve the appropriate OCT based on a fundus image belonging to the same eye. Furthermore, we showcase the transferability of the obtained representations by conducting linear probing and fine-tuning on several prediction tasks from OCT.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="bib:Mayr2023_2" class="col-sm-8">
    
      <div class="title">Boundary Graph Neural Networks for 3D Simulations</div>
      <div class="author">
        
          
            
              
                
                  Mayr, A.,
                
              
            
          
        
          
            
              
                
                  Lehner, S.,
                
              
            
          
        
          
            
              
                
                  Mayrhofer, A.,
                
              
            
          
        
          
            
              
                
                  Kloss, C.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Brandstetter, J.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2106.11299" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
      <a href="https://ml-jku.github.io/bgnn" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
    
    
      <a href="https://github.com/ml-jku/bgnn" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The abundance of data has given machine learning considerable momentum in natural sciences and engineering, though modeling of physical processes is often difficult. A particularly tough problem is the efficient representation of geometric boundaries. Triangularized geometric boundaries are well understood and ubiquitous in engineering applications. However, it is notoriously difficult to integrate them into machine learning approaches due to their heterogeneity with respect to size and orientation. In this work, we introduce an effective theory to model particle-boundary interactions, which leads to our new Boundary Graph Neural Networks (BGNNs) that dynamically modify graph structures to obey boundary conditions. The new BGNNs are tested on complex 3D granular flow processes of hoppers, rotating drums and mixers, which are all standard components of modern industrial machinery but still have complicated geometry. BGNNs are evaluated in terms of computational efficiency as well as prediction accuracy of particle flows and mixing entropies. BGNNs are able to accurately reproduce 3D granular flows within simulation uncertainties over hundreds of thousands of simulation timesteps. Most notably, in our experiments, particles stay within the geometric objects without using handcrafted conditions or restrictions.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AAAI</abbr>
    
  
  </div>

  <div id="bib:Mayr2023_1" class="col-sm-8">
    
      <div class="title">Boundary Graph Neural Networks for 3D Simulations</div>
      <div class="author">
        
          
            
              
                
                  Mayr, A.,
                
              
            
          
        
          
            
              
                
                  Lehner, S.,
                
              
            
          
        
          
            
              
                
                  Mayrhofer, A.,
                
              
            
          
        
          
            
              
                
                  Kloss, C.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Brandstetter, J.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26092" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
      <a href="https://ml-jku.github.io/bgnn" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
    
    
      <a href="https://github.com/ml-jku/bgnn" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The abundance of data has given machine learning considerable momentum in natural sciences and engineering, though modeling of physical processes is often difficult. A particularly tough problem is the efficient representation of geometric boundaries. Triangularized geometric boundaries are well understood and ubiquitous in engineering applications. However, it is notoriously difficult to integrate them into machine learning approaches due to their heterogeneity with respect to size and orientation. In this work, we introduce an effective theory to model particle-boundary interactions, which leads to our new Boundary Graph Neural Networks (BGNNs) that dynamically modify graph structures to obey boundary conditions. The new BGNNs are tested on complex 3D granular flow processes of hoppers, rotating drums and mixers, which are all standard components of modern industrial machinery but still have complicated geometry. BGNNs are evaluated in terms of computational efficiency as well as prediction accuracy of particle flows and mixing entropies. BGNNs are able to accurately reproduce 3D granular flows within simulation uncertainties over hundreds of thousands of simulation timesteps. Most notably, in our experiments, particles stay within the geometric objects without using handcrafted conditions or restrictions.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  </div>

  <div id="seidl2023clamp" class="col-sm-8">
    
      <div class="title">Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language</div>
      <div class="author">
        
          
            
              
                
                  Seidl, P.,
                
              
            
          
        
          
            
              
                
                  Vall, A.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the 40th International Conference on Machine Learning (ICML)</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/clamp" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  </div>

  <div id="patil2020alignrudder" class="col-sm-8">
    
      <div class="title">Align-RUDDER: Learning From Few Demonstrations by Reward Redistribution</div>
      <div class="author">
        
          
            
              
                
                  Patil, V.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  Dinu, M.,
                
              
            
          
        
          
            
              
                
                  Dorfer, M.,
                
              
            
          
        
          
            
              
                
                  Blies, P.,
                
              
            
          
        
          
            
              
                
                  Brandstetter, J.,
                
              
            
          
        
          
            
              
                
                  Arjona-Medina, J.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2009.14108</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2009.14108" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/align-rudder" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Reinforcement Learning algorithms require a large number of samples to solve complex tasks with sparse and delayed rewards. Complex tasks can often be hierarchically decomposed into sub-tasks. A step in the Q-function can be associated with solving a sub-task, where the expectation of the return increases. RUDDER has been introduced to identify these steps and then redistribute reward to them, thus immediately giving reward if sub-tasks are solved. Since the problem of delayed rewards is mitigated, learning is considerably sped up. However, for complex tasks, current exploration strategies as deployed in RUDDER struggle with discovering episodes with high rewards. Therefore, we assume that episodes with high rewards are given as demonstrations and do not have to be discovered by exploration. Typically the number of demonstrations is small and RUDDER’s LSTM model as a deep learning method does not learn well. Hence, we introduce Align-RUDDER, which is RUDDER with two major modifications. First, Align-RUDDER assumes that episodes with high rewards are given as demonstrations, replacing RUDDER’s safe exploration and lessons replay buffer. Second, we replace RUDDER’s LSTM model by a profile model that is obtained from multiple sequence alignment of demonstrations. Profile models can be constructed from as few as two demonstrations as known from bioinformatics. Align-RUDDER inherits the concept of reward redistribution, which considerably reduces the delay of rewards, thus speeding up learning. Align-RUDDER outperforms competitors on complex artificial tasks with delayed reward and few demonstrations. On the MineCraft ObtainDiamond task, Align-RUDDER is able to mine a diamond, though not frequently.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="paischer2022toward" class="col-sm-8">
    
      <div class="title">Toward Semantic History Compression for Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Paischer, F.,
                
              
            
          
        
          
            
              
                
                  Adler, T.,
                
              
            
          
        
          
            
              
                
                  Radler, A.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2022
      
      </div>
    

    <div class="links">
    
    
    
      <a href="https://openreview.net/forum?id=97C6klf5shp" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/helm" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ML4PS</abbr>
    
  
  </div>

  <div id="sanokows2022AVA" class="col-sm-8">
    
      <div class="title">One Network to Approximate Them All: Amortized Variational Inference of Ising Ground States</div>
      <div class="author">
        
          
            
              
                
                  Sanokowski, S.,
                
              
            
          
        
          
            
              
                
                  Berghammer, W.,
                
              
            
          
        
          
            
              
                
                  Johannes, K.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Lehner, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://ml4physicalsciences.github.io/2022/files/NeurIPS_ML4PS_2022_129.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>For a wide range of combinatorial optimization problems, finding the optimal solutions is equivalent to finding the ground states of corresponding Ising Hamiltonians. Recent work shows that these ground states are found more efficiently by variational approaches using autoregressive models than by traditional methods. In contrast to previous works, where for every problem instance a new model has to be trained, we aim at a single model that approximates the ground states for a whole family of Hamiltonians. We demonstrate that autoregregressive neural networks can be trained to achieve this goal and are able to generalize across a class of problems. We iteratively approximate the ground state based on a representation of the Hamiltonian that is provided by a graph neural network. Our experiments show that solving a large number of related problem instances by a single model can be considerably more efficient than solving them individually.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ML4PS</abbr>
    
  
  </div>

  <div id="tran2022using" class="col-sm-8">
    
      <div class="title">Using Shadows to Learn Ground State Properties of Quantum Hamiltonians</div>
      <div class="author">
        
          
            
              
                
                  Tran, V.,
                
              
            
          
        
          
            
              
                
                  Lewis, L.,
                
              
            
          
        
          
            
              
                
                  Huang, H.,
                
              
            
          
        
          
            
              
                
                  Kofler, J.,
                
              
            
          
        
          
            
              
                
                  Kueng, R.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Lehner, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://ml4physicalsciences.github.io/2022/files/NeurIPS_ML4PS_2022_184.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Predicting properties of the ground state of a given Hamiltonian is an important task central to various fields of science. Recent theoretical results show that for this task learning algorithms enjoy an advantage over non-learning algorithms for a wide range of important Hamiltonians. This work investigates whether the graph structure of these Hamiltonians can be leveraged for the design of sample efficient machine learning models. We demonstrate that corresponding Graph Neural Networks do indeed exhibit superior sample efficiency. Our results provide guidance in the design of machine learning models that learn on experimental data from near-term quantum devices.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">DeepRL</abbr>
    
  
  </div>

  <div id="siripurapu2022infodist" class="col-sm-8">
    
      <div class="title">InfODist: Online distillation with Informative rewards improves generalization in Curriculum Learning</div>
      <div class="author">
        
          
            
              
                
                  Siripurapu, R.,
                
              
            
          
        
          
            
              
                
                  Patil, V.,
                
              
            
          
        
          
            
              
                
                  Schweighofer, K.,
                
              
            
          
        
          
            
              
                
                  Dinu, M.,
                
              
            
          
        
          
            
              
                
                  Schmied, T.,
                
              
            
          
        
          
            
              
                
                  Diez, L.,
                
              
            
          
        
          
            
              
                
                  Holzleitner, M.,
                
              
            
          
        
          
            
              
                
                  Eghbal-zadeh, H.,
                
              
            
          
        
          
            
              
                
                  Kopp, M.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://openreview.net/forum?id=9CvMkA8oi8O" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Curriculum learning (CL) is an essential part of human learning, just as reinforcement learning (RL) is. However, CL agents that are trained using RL with neural networks produce limited generalization to later tasks in the curriculum. We show that online distillation using learned informative rewards tackles this problem. Here, we consider a reward to be informative if it is positive when the agent makes progress towards the goal and negative otherwise. Thus, an informative reward allows an agent to learn immediately to avoid states which are irrelevant to the task. And, the value and policy networks do not utilize their limited capacity to fit targets for these irrelevant states. Consequently, this improves generalization to later tasks. Our contributions: First, we propose InfODist, an online distillation method that makes use of informative rewards to significantly improve generalization in CL. Second, we show that training with informative rewards ameliorates the capacity loss phenomenon that was previously attributed to non-stationarities during the training process. Third, we show that learning from task-irrelevant states explains the capacity loss and subsequent impaired generalization. In conclusion, our work is a crucial step toward scaling curriculum learning to complex real world tasks</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">FMDM</abbr>
    
  
  </div>

  <div id="paischer2022foundation" class="col-sm-8">
    
      <div class="title">Foundation Models for History Compression in Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Paischer, F.,
                
              
            
          
        
          
            
              
                
                  Adler, T.,
                
              
            
          
        
          
            
              
                
                  Radler, A.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://openreview.net/forum?id=KmxQ3v7nMmd" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/helm" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Agents interacting under partial observability require access to past observations via a memory mechanism in order to approximate the true state of the environment. Recent work suggests that leveraging language as abstraction provides benefits for creating a representation of past events. History Compression via Language Models (HELM) leverages a pretrained Language Model (LM) for representing the past. It relies on a randomized attention mechanism to translate environment observations to token embeddings. In this work, we show that the representations resulting from this attention mechanism can collapse under certain conditions. This causes blindness of the agent to subtle changes in the environment that may be crucial for solving a certain task. We propose a solution to this problem consisting of two parts. First, we improve upon HELM by substituting the attention mechanism with a feature-wise centering-and-scaling operation. Second, we take a step toward semantic history compression by leveraging foundation models, such as CLIP, to encode observations, which further improves performance. By combining foundation models, our agent is able to solve the challenging MiniGrid-Memory environment. Surprisingly, however, our experiments suggest that this is not due to the semantic enrichment of the representation presented to the LM, but rather due to the discriminative power provided by CLIP. We make our code publicly available at https://github.com/ml-jku/helm.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AI4Science</abbr>
    
  
  </div>

  <div id="svensson2022robust" class="col-sm-8">
    
      <div class="title">Robust task-specific adaption of models for drug-target interaction prediction</div>
      <div class="author">
        
          
            
              
                
                  Svensson, E.,
                
              
            
          
        
          
            
              
                
                  Hoedt, P.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In NeurIPS 2022 AI for Science: Progress and Promises</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://openreview.net/pdf?id=dIX34JWnIAL" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/hyper-dti" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>HyperNetworks have been established as an effective technique to achieve fast adaptation of parameters for neural networks. Recently, HyperNetworks conditioned on descriptors of tasks have improved multi-task generalization in various domains, such as personalized federated learning and neural architecture search. Especially powerful results were achieved in few- and zero-shot settings, attributed to the increased information sharing by the HyperNetwork. With the rise of new diseases fast discovery of drugs is needed which requires proteo-chemometric models that are able to generalize drug-target interaction predictions in low-data scenarios. State-of-the-art methods apply a few fully-connected layers to concatenated learned embeddings of the protein target and drug compound. In this work, we develop a task-conditioned HyperNetwork approach for the problem of predicting drug-target interactions in drug discovery. We show that when model parameters are predicted for the fully-connected layers processing the drug compound embedding, based on the protein target embedding, predictive performance can be improved over previous methods. Two additional components of our architecture, a) switching to L1 loss, and b) integrating a context module for proteins, further boost performance and robustness. On an established benchmark for proteo-chemometrics models, our architecture outperforms previous methods in all settings, including few- and zero-shot settings. In an ablation study, we analyze the importance of each of the components of our HyperNetwork approach.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ML4Molecules</abbr>
    
  
  </div>

  <div id="svensson2022task" class="col-sm-8">
    
      <div class="title">Task-conditioned modeling of drug-target interactions</div>
      <div class="author">
        
          
            
              
                
                  Svensson, E.,
                
              
            
          
        
          
            
              
                
                  Hoedt, P.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ELLIS Machine Learning for Molecule Discovery Workshop</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://openreview.net/pdf?id=MrUwwGKRhOM" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/hyper-dti" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>HyperNetworks have been established as an effective technique to achieve fast adaptation of parameters for neural networks. Recently, HyperNetworks conditioned on descriptors of tasks have improved multi-task generalization in various domains, such as personalized federated learning and neural architecture search. Especially powerful results were achieved in few- and zero-shot settings, attributed to the increased information sharing by the HyperNetwork. With the rise of new diseases fast discovery of drugs is needed which requires proteo-chemometric models that are able to generalize drug-target interaction predictions in low-data scenarios. State-of-the-art methods apply a few fully-connected layers to concatenated learned embeddings of the protein target and drug compound. In this work, we develop a task-conditioned HyperNetwork approach for the problem of predicting drug-target interactions in drug discovery. We show that when model parameters are predicted for the fully-connected layers processing the drug compound embedding, based on the protein target embedding, predictive performance can be improved over previous methods. Two additional components of our architecture, a) switching to L1 loss, and b) integrating a context module for proteins, further boost performance and robustness. On an established benchmark for proteo-chemometrics models, our architecture outperforms previous methods in all settings, including few- and zero-shot settings. In an ablation study, we analyze the importance of each of the components of our HyperNetwork approach.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CoLLAs</abbr>
    
  
  </div>

  <div id="schweighofer2022dataset" class="col-sm-8">
    
      <div class="title">A Dataset Perspective on Offline Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Schweighofer, K.,
                
              
            
          
        
          
            
              
                
                  Radler, A.,
                
              
            
          
        
          
            
              
                
                  Dinu, M.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  Patil, V.,
                
              
            
          
        
          
            
              
                
                  Bitto-Nemling, A.,
                
              
            
          
        
          
            
              
                
                  Eghbal-zadeh, H.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2111.04714" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
      <a href="https://kschweig.github.io/OfflineRL/" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
    
    
      <a href="https://github.com/ml-jku/OfflineRL" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The application of Reinforcement Learning (RL) in real world environments can be expensive or risky due to sub-optimal policies during training. In Offline RL, this problem is avoided since interactions with an environment are prohibited. Policies are learned from a given dataset, which solely determines their performance. Despite this fact, how dataset characteristics influence Offline RL algorithms is still hardly investigated. The dataset characteristics are determined by the behavioral policy that samples this dataset. Therefore, we define characteristics of behavioral policies as exploratory for yielding high expected information in their interaction with the Markov Decision Process (MDP) and as exploitative for having high expected return. We implement two corresponding empirical measures for the datasets sampled by the behavioral policy in deterministic MDPs. The first empirical measure SACo is defined by the normalized unique state-action pairs and captures exploration. The second empirical measure TQ is defined by the normalized average trajectory return and captures exploitation. Empirical evaluations show the effectiveness of TQ and SACo. In large-scale experiments using our proposed measures, we show that the unconstrained off-policy Deep Q-Network family requires datasets with high SACo to find a good policy. Furthermore, experiments show that policy constraint algorithms perform well on datasets with high TQ and SACo. Finally, the experiments show, that purely dataset-constrained Behavioral Cloning performs competitively to the best Offline RL algorithms for datasets with high TQ.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CoLLAs</abbr>
    
  
  </div>

  <div id="steinparz2022reactive" class="col-sm-8">
    
      <div class="title">Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Steinparz, C.,
                
              
            
          
        
          
            
              
                
                  Schmied, T.,
                
              
            
          
        
          
            
              
                
                  Paischer, F.,
                
              
            
          
        
          
            
              
                
                  Dinu, M.,
                
              
            
          
        
          
            
              
                
                  Patil, V.,
                
              
            
          
        
          
            
              
                
                  Bitto-Nemling, A.,
                
              
            
          
        
          
            
              
                
                  Eghbal-zadeh, H.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2207.05742" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/reactive-exploration" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In lifelong learning, an agent learns throughout its entire life without resets, in a constantly changing environment, as we humans do. Consequently, lifelong learning comes with a plethora of research problems such as continual domain shifts, which result in non-stationary rewards and environment dynamics. These non-stationarities are difficult to detect and cope with due to their continuous nature. Therefore, exploration strategies and learning methods are required that are capable of tracking the steady domain shifts, and adapting to them. We propose Reactive Exploration to track and react to continual domain shifts in lifelong reinforcement learning, and to update the policy correspondingly. To this end, we conduct experiments in order to investigate different exploration strategies. We empirically show that representatives of the policy-gradient family are better suited for lifelong learning, as they adapt more quickly to distribution shifts than Q-learning. Thereby, policy-gradient methods profit the most from Reactive Exploration and show good results in lifelong learning with continual domain shifts.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="schaefl2022hopular" class="col-sm-8">
    
      <div class="title">Hopular: Modern Hopfield Networks for Tabular Data</div>
      <div class="author">
        
          
            
              
                
                  Schäfl, B.,
                
              
            
          
        
          
            
              
                
                  Gruber, L.,
                
              
            
          
        
          
            
              
                
                  Bitto-Nemling, A.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2206.00664" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
      <a href="https://ml-jku.github.io/hopular/" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
    
    
      <a href="https://github.com/ml-jku/hopular" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While Deep Learning excels in structured data as encountered in vision and natural language processing, it failed to meet its expectations on tabular data. For tabular data, Support Vector Machines (SVMs), Random Forests, and Gradient Boosting are the best performing techniques with Gradient Boosting in the lead. Recently, we saw a surge of Deep Learning methods that were tailored to tabular data but still underperform compared to Gradient Boosting on small-sized datasets. We suggest "Hopular", a novel Deep Learning architecture for medium- and small-sized datasets, where each layer is equipped with continuous modern Hopfield networks. The modern Hopfield networks use stored data to identify feature-feature, feature-target, and sample-sample dependencies. Hopular’s novelty is that every layer can directly access the original input as well as the whole training set via stored data in the Hopfield networks. Therefore, Hopular can step-wise update its current model and the resulting prediction at every layer like standard iterative learning algorithms. In experiments on small-sized tabular datasets with less than 1,000 samples, Hopular surpasses Gradient Boosting, Random Forests, SVMs, and in particular several Deep Learning methods. In experiments on medium-sized tabular data with about 10,000 samples, Hopular outperforms XGBoost, CatBoost, LightGBM and a state-of-the art Deep Learning method designed for tabular data. Thus, Hopular is a strong alternative to these methods on tabular data.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CoLLAs</abbr>
    
  
  </div>

  <div id="gauch2022subgd" class="col-sm-8">
    
      <div class="title">Few-Shot Learning by Dimensionality Reduction in Gradient Space</div>
      <div class="author">
        
          
            
              
                
                  Gauch, M.,
                
              
            
          
        
          
            
              
                
                  Beck, M.,
                
              
            
          
        
          
            
              
                
                  Adler, T.,
                
              
            
          
        
          
            
              
                
                  Kotsur, D.,
                
              
            
          
        
          
            
              
                
                  Fiel, S.,
                
              
            
          
        
          
            
              
                
                  Eghbal-zadeh, H.,
                
              
            
          
        
          
            
              
                
                  Brandstetter, J.,
                
              
            
          
        
          
            
              
                
                  Kofler, J.,
                
              
            
          
        
          
            
              
                
                  Holzleitner, M.,
                
              
            
          
        
          
            
              
                
                  Zellinger, W.,
                
              
            
          
        
          
            
              
                
                  Klotz, D.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Lehner, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2206.03483" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
      <a href="https://ml-jku.github.io/subgd/" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
    
    
      <a href="https://github.com/ml-jku/subgd" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We introduce SubGD, a novel few-shot learning method which is based on the recent finding that stochastic gradient descent updates tend to live in a low-dimensional parameter subspace. In experimental and theoretical analyses, we show that models confined to a suitable predefined subspace generalize well for few-shot learning. A suitable subspace fulfills three criteria across the given tasks: it (a) allows to reduce the training error by gradient flow, (b) leads to models that generalize well, and (c) can be identified by stochastic gradient descent. SubGD identifies these subspaces from an eigendecomposition of the auto-correlation matrix of update directions across different tasks. Demonstrably, we can identify low-dimensional suitable subspaces for few-shot learning of dynamical systems, which have varying properties described by one or few parameters of the analytical system description. Such systems are ubiquitous among real-world applications in science and engineering. We experimentally corroborate the advantages of SubGD on three distinct dynamical systems problem settings, significantly outperforming popular few-shot learning methods both in terms of sample efficiency and performance.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  </div>

  <div id="paischer2022helm" class="col-sm-8">
    
      <div class="title">History Compression via Language Models in Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Paischer, F.,
                
              
            
          
        
          
            
              
                
                  Adler, T.,
                
              
            
          
        
          
            
              
                
                  Patil, V.,
                
              
            
          
        
          
            
              
                
                  Bitto-Nemling, A.,
                
              
            
          
        
          
            
              
                
                  Holzleitner, M.,
                
              
            
          
        
          
            
              
                
                  Lehner, S.,
                
              
            
          
        
          
            
              
                
                  Eghbal-zadeh, H.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In </em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2205.12258" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
      <a href="https://ml-jku.github.io/blog/2022/helm/" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
    
    
      <a href="https://github.com/ml-jku/helm" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In a partially observable Markov decision process (POMDP), an agent typically uses a representation of the past to approximate the underlying MDP. We propose to utilize a frozen Pretrained Language Transformer (PLT) for history representation and compression to improve sample efficiency. To avoid training of the Transformer, we introduce FrozenHopfield, which automatically associates observations with original token embeddings. To form these associations, a modern Hopfield network stores the original token embeddings, which are retrieved by queries that are obtained by a random but fixed projection of observations. Our new method, HELM, enables actor-critic network architectures that contain a pretrained language Transformer for history representation as a memory module. Since a representation of the past need not be learned, HELM is much more sample efficient than competitors. On Minigrid and Procgen environments HELM achieves new state-of-the-art results. Our code is available at https://github.com/ml-jku/helm.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">JCIM</abbr>
    
  
  </div>

  <div id="seidlImproving2022" class="col-sm-8">
    
      <div class="title">Improving Few- and Zero-Shot Reaction Template Prediction Using Modern Hopfield Networks</div>
      <div class="author">
        
          
            
              
                
                  Seidl, P.,
                
              
            
          
        
          
            
              
                
                  Renz, P.,
                
              
            
          
        
          
            
              
                
                  Dyubankova, N.,
                
              
            
          
        
          
            
              
                
                  Neves, P.,
                
              
            
          
        
          
            
              
                
                  Verhoeven, J.,
                
              
            
          
        
          
            
              
                
                  Wegner, J.,
                
              
            
          
        
          
            
              
                
                  Segler, M.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Journal of Chemical Information and Modeling</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1021/acs.jcim.1c01065" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/mhn-react" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Finding synthesis routes for molecules of interest is essential in the discovery of new drugs and materials. To find such routes, computer-assisted synthesis planning (CASP) methods are employed, which rely on a single-step model of chemical reactivity. In this study, we introduce a template-based single-step retrosynthesis model based on Modern Hopfield Networks, which learn an encoding of both molecules and reaction templates in order to predict the relevance of templates for a given molecule. The template representation allows generalization across different reactions and significantly improves the performance of template relevance prediction, especially for templates with few or zero training examples. With inference speed up to orders of magnitude faster than baseline methods, we improve or match the state-of-the-art performance for top-k exact match accuracy for k ≥ 3 in the retrosynthesis benchmark USPTO-50k. Code to reproduce the results is available at github.com/ml-jku/mhn-react.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  </div>

  <div id="hoedt2022normalization" class="col-sm-8">
    
      <div class="title">Normalization is dead, long live normalization!</div>
      <div class="author">
        
          
            
              
                
                  Hoedt, P.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ICLR Blog Track</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://iclr-blog-track.github.io/2022/03/25/unnormalized-resnets/" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Since the advent of Batch Normalization (BN), almost every state-of-the-art (SOTA) method uses some form of normalization. After all, normalization generally speeds up learning and leads to models that generalize better than their unnormalized counterparts. This turns out to be especially useful when using some form of skip connections, which are prominent in Residual Networks (ResNets), for example. However, Brock et al. (2021a) suggest that SOTA performance can also be achieved using ResNets without normalization!</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div>

  <div id="pmlr-v220-caggiano23a" class="col-sm-8">
    
      <div class="title">MyoChallenge 2022: Learning contact-rich manipulation using a musculoskeletal hand</div>
      <div class="author">
        
          
            
              
                
                  Caggiano, V.,
                
              
            
          
        
          
            
              
                
                  Durandau, G.,
                
              
            
          
        
          
            
              
                
                  Wang, H.,
                
              
            
          
        
          
            
              
                
                  Chiappa, A.,
                
              
            
          
        
          
            
              
                
                  Mathis, A.,
                
              
            
          
        
          
            
              
                
                  Tano, P.,
                
              
            
          
        
          
            
              
                
                  Patel, N.,
                
              
            
          
        
          
            
              
                
                  Pouget, A.,
                
              
            
          
        
          
            
              
                
                  Schumacher, P.,
                
              
            
          
        
          
            
              
                
                  Martius, G.,
                
              
            
          
        
          
            
              
                
                  Haeufle, D.,
                
              
            
          
        
          
            
              
                
                  Geng, Y.,
                
              
            
          
        
          
            
              
                
                  An, B.,
                
              
            
          
        
          
            
              
                
                  Zhong, Y.,
                
              
            
          
        
          
            
              
                
                  Ji, J.,
                
              
            
          
        
          
            
              
                
                  Chen, Y.,
                
              
            
          
        
          
            
              
                
                  Dong, H.,
                
              
            
          
        
          
            
              
                
                  Yang, Y.,
                
              
            
          
        
          
            
              
                
                  Siripurapu, R.,
                
              
            
          
        
          
            
              
                
                  Ferro Diez, L.,
                
              
            
          
        
          
            
              
                
                  Kopp, M.,
                
              
            
          
        
          
            
              
                
                  Patil, V.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  Tassa, Y.,
                
              
            
          
        
          
            
              
                
                  Merel, J.,
                
              
            
          
        
          
            
              
                
                  Schultheis, R.,
                
              
            
          
        
          
            
              
                
                  Song, S.,
                
              
            
          
        
          
            
              
                
                  Sartori, M.,
                
              
            
          
        
          
            
              
                
                  and Kumar, V.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the NeurIPS 2022 Competitions Track</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://proceedings.mlr.press/v220/caggiano23a.html" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
      
      <a href="https://proceedings.mlr.press/v220/caggiano23a/caggiano23a.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Manual dexterity has been considered one of the critical components for human evolution. The ability to perform movements as simple as holding and rotating an object in the hand without dropping it needs the coordination of more than 35 muscles which act synergistically or antagonistically on multiple joints. This complexity in control is markedly different from typical pre-specified movements or torque based controls used in robotics. In the MyoChallenge at the NeurIPS 2022 competition track, we challenged the community to develop controllers for a realistic hand to solve a series of dexterous manipulation tasks. The MyoSuite framework was used to train and test controllers on realistic, contact rich and computation efficient virtual neuromusculoskeletal model of the hand and wrist. Two tasks were proposed: a die re-orientation and a boading ball (rotation of two spheres respect to each other) tasks. More than 40 teams participated to the challenge and submitted more than 340 solutions. The challenge was split in two phases. In the first phase, where a limited set of objectives and randomization were proposed, teams managed to achieve high performance, in particular in the boading-ball task.  In the second phase as the focus shifted towards generalization of task solutions to extensive variations of object and task properties, teams saw significant performance drop. This shows that there is still a large gap in developing agents capable of generalizable skilled manipulation. In future challenges, we will continue pursuing the generalizability both in skills and agility of the tasks exploring additional realistic neuromusculoskeletal models.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  </div>

  <div id="hoedtMCLSTM2021" class="col-sm-8">
    
      <div class="title">MC-LSTM: Mass-Conserving LSTM</div>
      <div class="author">
        
          
            
              
                
                  Hoedt, P.,
                
              
            
          
        
          
            
              
                
                  Kratzert, F.,
                
              
            
          
        
          
            
              
                
                  Klotz, D.,
                
              
            
          
        
          
            
              
                
                  Halmich, C.,
                
              
            
          
        
          
            
              
                
                  Holzleitner, M.,
                
              
            
          
        
          
            
              
                
                  Nearing, G.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 38th International Conference on Machine Learning</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://proceedings.mlr.press/v139/hoedt21a.html" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
      
      <a href="https://proceedings.mlr.press/v139/hoedt21a/hoedt21a.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/ml-jku/mc-lstm" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The success of Convolutional Neural Networks (CNNs) in computer vision is mainly driven by their strong inductive bias, which is strong enough to allow CNNs to solve vision-related tasks with random weights, meaning without learning. Similarly, Long Short-Term Memory (LSTM) has a strong inductive bias towards storing information over time. However, many real-world systems are governed by conservation laws, which lead to the redistribution of particular quantities – e.g. in physical and economical systems. Our novel Mass-Conserving LSTM (MC-LSTM) adheres to these conservation laws by extending the inductive bias of LSTM to model the redistribution of those stored quantities. MC-LSTMs set a new state-of-the-art for neural arithmetic units at learning arithmetic operations, such as addition tasks, which have a strong conservation law, as the sum is constant over time. Further, MC-LSTM is applied to traffic forecasting, modelling a pendulum, and a large benchmark dataset in hydrology, where it sets a new state-of-the-art for predicting peak flows. In the hydrology example, we show that MC-LSTM states correlate with real-world processes and are therefore interpretable.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="widrich2021modern" class="col-sm-8">
    
      <div class="title">Modern Hopfield Networks for Return Decomposition for Delayed Rewards</div>
      <div class="author">
        
          
            
              
                
                  Widrich, M.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  Patil, V.,
                
              
            
          
        
          
            
              
                
                  Bitto-Nemling, A.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Deep RL Workshop NeurIPS 2021</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://openreview.net/forum?id=t0PQSDcqAiy" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Delayed rewards, which are separated from their causative actions by irrelevant actions, hamper learning in reinforcement learning (RL). Especially real world problems often contain such delayed and sparse rewards. Recently, return decomposition for delayed rewards (RUDDER) employed pattern recognition to remove or reduce delay in rewards, which dramatically simplifies the learning task of the underlying RL method. RUDDER was realized using a long short-term memory (LSTM). The LSTM was trained to identify important state-action pair patterns, responsible for the return. Reward was then redistributed to these important state-action pairs. However, training the LSTM is often difficult and requires a large number of episodes. In this work, we replace the LSTM with the recently proposed continuous modern Hopfield networks (MHN) and introduce Hopfield-RUDDER. MHN are powerful trainable associative memories with large storage capacity. They require only few training samples and excel at identifying and recognizing patterns. We use this property of MHN to identify important state-action pairs that are associated with low or high return episodes and directly redistribute reward to them. However, in partially observable environments, Hopfield-RUDDER requires additional information about the history of state-action pairs. Therefore, we evaluate several methods for compressing history and introduce reset-max history, a lightweight history compression using the max-operator in combination with a reset gate. We experimentally show that Hopfield-RUDDER is able to outperform LSTM-based RUDDER on various 1D environments with small numbers of episodes. Finally, we show in preliminary experiments that Hopfield-RUDDER scales to highly complex environments with the Minecraft ObtainDiamond task from the MineRL NeurIPS challenge.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="schweighofer:21" class="col-sm-8">
    
      <div class="title">Understanding the Effects of Dataset Characteristics on Offline Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Schweighofer, K.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  Dinu, M.,
                
              
            
          
        
          
            
              
                
                  Renz, P.,
                
              
            
          
        
          
            
              
                
                  Bitto-Nemling, A.,
                
              
            
          
        
          
            
              
                
                  Patil, V.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2111.04714" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/OfflineRL" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In real world, affecting the environment by a weak policy can be expensive or very risky, therefore hampers real world applications of reinforcement learning. Offline Reinforcement Learning (RL) can learn policies from a given dataset without interacting with the environment. However, the dataset is the only source of information for an Offline RL algorithm and determines the performance of the learned policy. We still lack studies on how dataset characteristics influence different Offline RL algorithms. Therefore, we conducted a comprehensive empirical analysis of how dataset characteristics effect the performance of Offline RL algorithms for discrete action environments. A dataset is characterized by two metrics: (1) the average dataset return measured by the Trajectory Quality (TQ) and (2) the coverage measured by the State-Action Coverage (SACo). We found that variants of the off-policy Deep Q-Network family require datasets with high SACo to perform well. Algorithms that constrain the learned policy towards the given dataset perform well for datasets with high TQ or SACo. For datasets with high TQ, Behavior Cloning outperforms or performs similarly to the best Offline RL algorithms.
</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="fürst2021cloob" class="col-sm-8">
    
      <div class="title">CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP</div>
      <div class="author">
        
          
            
              
                
                  Fürst, A.,
                
              
            
          
        
          
            
              
                
                  Rumetshofer, E.,
                
              
            
          
        
          
            
              
                
                  Tran, V.,
                
              
            
          
        
          
            
              
                
                  Ramsauer, H.,
                
              
            
          
        
          
            
              
                
                  Tang, F.,
                
              
            
          
        
          
            
              
                
                  Lehner, J.,
                
              
            
          
        
          
            
              
                
                  Kreil, D.,
                
              
            
          
        
          
            
              
                
                  Kopp, M.,
                
              
            
          
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  Bitto-Nemling, A.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2110.11316" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
      <a href="https://ml-jku.github.io/cloob/" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
    
    
      <a href="https://github.com/ml-jku/cloob" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Contrastive learning with the InfoNCE objective is exceptionally successful in various self-supervised learning tasks. Recently, the CLIP model yielded impressive results on zero-shot transfer learning when using InfoNCE for learning visual representations from natural language supervision. However, InfoNCE as a lower bound on the mutual information has been shown to perform poorly for high mutual information. In contrast, the InfoLOOB upper bound (leave one out bound) works well for high mutual information but suffers from large variance and instabilities. We introduce "Contrastive Leave One Out Boost" (CLOOB), where modern Hopfield networks boost learning with the InfoLOOB objective. Modern Hopfield networks replace the original embeddings by retrieved embeddings in the InfoLOOB objective. The retrieved embeddings give InfoLOOB two assets. Firstly, the retrieved embeddings stabilize InfoLOOB, since they are less noisy and more similar to one another than the original embeddings. Secondly, they are enriched by correlations, since the covariance structure of embeddings is reinforced through retrievals. We compare CLOOB to CLIP after learning on the Conceptual Captions and the YFCC dataset with respect to their zero-shot transfer learning performance on other datasets. CLOOB consistently outperforms CLIP at zero-shot transfer learning across all considered architectures and datasets.
</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="mayr2021_l3dgfs" class="col-sm-8">
    
      <div class="title">Learning 3D Granular Flow Simulations</div>
      <div class="author">
        
          
            
              
                
                  Mayr, A.,
                
              
            
          
        
          
            
              
                
                  Lehner, S.,
                
              
            
          
        
          
            
              
                
                  Mayrhofer, A.,
                
              
            
          
        
          
            
              
                
                  Kloss, C.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Brandstetter, J.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2105.01636" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
      
      <a href="https://simdl.github.io/posters/42-supp_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recently, the application of machine learning models has gained momentum in natural sciences and engineering, which is a natural fit due to the abundance of data in these fields. However, the modeling of physical processes from simulation data without first principle solutions remains difficult. Here, we present a Graph Neural Networks approach towards accurate modeling of complex 3D granular flow simulation processes created by the discrete element method LIGGGHTS and concentrate on simulations of physical systems found in real world applications like rotating drums and hoppers. We discuss how to implement Graph Neural Networks that deal with 3D objects, boundary conditions, particle - particle, and particle - boundary interactions such that an accurate modeling of relevant physical quantities is made possible. Finally, we compare the machine learning based trajectories to LIGGGHTS trajectories in terms of particle flows and mixing entropies.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">HESSD</abbr>
    
  
  </div>

  <div id="klotz2021uncertainty" class="col-sm-8">
    
      <div class="title">Uncertainty Estimation with Deep Learning for Rainfall-Runoff Modelling</div>
      <div class="author">
        
          
            
              
                
                  Klotz, D.,
                
              
            
          
        
          
            
              
                
                  Kratzert, F.,
                
              
            
          
        
          
            
              
                
                  Gauch, M.,
                
              
            
          
        
          
            
              
                
                  Keefe Sampson, A.,
                
              
            
          
        
          
            
              
                
                  Brandstetter, J.,
                
              
            
          
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Nearing, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Hydrology and Earth System Sciences Discussions</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://hess.copernicus.org/preprints/hess-2021-154/" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/neuralhydrology/neuralhydrology" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Deep Learning is becoming an increasingly important way to produce accurate hydrological predictions across a wide range of spatial and temporal scales. Uncertainty estimations are critical for actionable hydrological forecasting, and while standardized community benchmarks are becoming an increasingly important part of hydrological model development and research, similar tools for benchmarking uncertainty estimation are lacking. This contributions demonstrates that accurate uncertainty predictions can be obtained with Deep Learning. We establish an uncertainty estimation benchmarking procedure and present four Deep Learning baselines. Three baselines are based on Mixture Density Networks and one is based on Monte Carlo dropout. The results indicate that these approaches constitute strong baselines, especially the former ones. Additionaly, we provide a post-hoc model analysis to put forward some qualitative understanding of the resulting models. This analysis extends the notion of performance and show that learn nuanced behaviors in different situations.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">HESS</abbr>
    
  
  </div>

  <div id="kratzert2021synergy" class="col-sm-8">
    
      <div class="title">A note on leveraging synergy in multiple meteorological datasets with deep learning for rainfall-runoff modeling</div>
      <div class="author">
        
          
            
              
                
                  Kratzert, F.,
                
              
            
          
        
          
            
              
                
                  Klotz, D.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Nearing, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Hydrology and Earth System Sciences</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://hess.copernicus.org/preprints/hess-2020-221/" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/kratzert/multiple_forcing" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>A deep learning rainfall-runoff model can take multiple meteorological forcing products as inputs and learn to combine them in spatially and temporally dynamic ways. This is demonstrated using Long Short Term Memory networks (LSTMs) trained over basins in the continental US using the CAMELS data set. Using multiple precipitation products (NLDAS, Maurer, DayMet) in a single LSTM significantly improved simulation accuracy relative to using only individual precipitation products. A sensitivity analysis showed that the LSTM learned to utilize different precipitation products in different ways in different basins and for simulating different parts of the hydrograph in individual basins.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">HESS</abbr>
    
  
  </div>

  <div id="gauch2021mtslstm" class="col-sm-8">
    
      <div class="title">Rainfall–runoff prediction at multiple timescales with a single Long Short-Term Memory network</div>
      <div class="author">
        
          
            
              
                
                  Gauch, M.,
                
              
            
          
        
          
            
              
                
                  Kratzert, F.,
                
              
            
          
        
          
            
              
                
                  Klotz, D.,
                
              
            
          
        
          
            
              
                
                  Nearing, G.,
                
              
            
          
        
          
            
              
                
                  Lin, J.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Hydrology and Earth System Sciences</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://hess.copernicus.org/articles/25/2045/2021/" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/gauchm/mts-lstm" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Long Short-Term Memory (LSTM) networks have been applied to daily discharge prediction with remarkable success. Many practical applications, however, require predictions at more granular timescales. For instance, accurate prediction of short but extreme flood peaks can make a lifesaving difference, yet such peaks may escape the coarse temporal resolution of daily predictions. Naively training an LSTM on hourly data, however, entails very long input sequences that make learning difficult and computationally expensive. In this study, we propose two multi-timescale LSTM (MTS-LSTM) architectures that jointly predict multiple timescales within one model, as they process long-past inputs at a different temporal resolution than more recent inputs. In a benchmark on 516 basins across the continental United States, these models achieved significantly higher Nash–Sutcliffe efficiency (NSE) values than the US National Water Model. Compared to naive prediction with distinct LSTMs per timescale, the multi-timescale architectures are computationally more efficient with no loss in accuracy. Beyond prediction quality, the multi-timescale LSTM can process different input variables at different timescales, which is especially relevant to operational applications where the lead time of meteorological forcings depends on their temporal resolution.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Frontiers in AI</abbr>
    
  
  </div>

  <div id="vall_promise_2021" class="col-sm-8">
    
      <div class="title">The Promise of AI for DILI Prediction</div>
      <div class="author">
        
          
            
              
                
                  Vall, A.,
                
              
            
          
        
          
            
              
                
                  Sabnis, Y.,
                
              
            
          
        
          
            
              
                
                  Shi, J.,
                
              
            
          
        
          
            
              
                
                  Class, R.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Frontiers in Artificial Intelligence</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://www.frontiersin.org/articles/10.3389/frai.2021.638410" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Drug-induced liver injury (DILI) is a common reason for the withdrawal of a drug from the market. Early assessment of DILI risk is an essential part of drug development, but it is rendered challenging prior to clinical trials by the complex factors that give rise to liver damage. Artificial intelligence (AI) approaches, particularly those building on machine learning, range from random forests to more recent techniques such as deep learning, and provide tools that can analyze chemical compounds and accurately predict some of their properties based purely on their structure. This article reviews existing AI approaches to predicting DILI and elaborates on the challenges that arise from the as yet limited availability of data. Future directions are discussed focusing on rich data modalities, such as 3D spheroids, and the slow but steady increase in drugs annotated with DILI risk labels.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">medRxiv</abbr>
    
  
  </div>

  <div id="roland2021covid" class="col-sm-8">
    
      <div class="title">Machine Learning based COVID-19 Diagnosis from Blood Tests with Robustness to Domain Shifts</div>
      <div class="author">
        
      </div>

      <div class="periodical">
      
        <em>medRxiv preprint doi:10.1101/2021.04.06.21254997</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://www.medrxiv.org/content/10.1101/2021.04.06.21254997v1.full-text" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We investigate machine learning models that identify COVID-19 positive patients and estimate the mortality risk based on routinely acquired blood tests in a hospital setting. However, during pandemics or new outbreaks, disease and testing characteristics change, thus we face domain shifts. Domain shifts can be caused, e.g., by changes in the disease prevalence (spreading or tested population), by refined RT-PCR testing procedures (taking samples, laboratory), or by virus mutations. Therefore, machine learning models for diagnosing COVID-19 or other diseases may not be reliable and degrade in performance over time. To countermand this effect, we propose methods that first identify domain shifts and then reverse their negative effects on the model performance. Frequent re-training and reassessment, as well as stronger weighting of more recent samples, keeps model performance and credibility at a high level over time. Our diagnosis models are constructed and tested on large-scale data sets, steadily adapt to observed domain shifts, and maintain high ROC AUC values along pandemics.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="seidl2021reaction" class="col-sm-8">
    
      <div class="title">Modern Hopfield Networks for Few- and Zero-Shot Reaction Prediction</div>
      <div class="author">
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2104.03279</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>An essential step in the discovery of new drugs and materials is the synthesis of a molecule that exists so far only as an idea to test its biological and physical properties. While computer-aided design of virtual molecules has made large progress, computer-assisted synthesis planning (CASP) to realize physical molecules is still in its infancy and lacks a performance level that would enable large-scale molecule discovery. CASP supports the search for multi-step synthesis routes, which is very challenging due to high branching factors in each synthesis step and the hidden rules that govern the reactions. The central and repeatedly applied step in CASP is reaction prediction, for which machine learning methods yield the best performance. We propose a novel reaction prediction approach that uses a deep learning architecture with modern Hopfield networks (MHNs) that is optimized by contrastive learning. An MHN is an associative memory that can store and retrieve chemical reactions in each layer of a deep learning architecture. We show that our MHN contrastive learning approach enables few- and zero-shot learning for reaction prediction which, in contrast to previous methods, can deal with rare, single, or even no training example(s) for a reaction. On a well established benchmark, our MHN approach pushes the state-of-the-art performance up by a large margin as it improves the predictive top-100 accuracy from 0.858±0.004 to 0.959±0.004. This advance might pave the way to large-scale molecule discovery.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="winterTrustedAI2021" class="col-sm-8">
    
      <div class="title">Trusted Artificial Intelligence: Towards Certification of Machine Learning Applications</div>
      <div class="author">
        
          
            
              
                
                  Winter, P.,
                
              
            
          
        
          
            
              
                
                  Eder, S.,
                
              
            
          
        
          
            
              
                
                  Weissenböck, J.,
                
              
            
          
        
          
            
              
                
                  Schwald, C.,
                
              
            
          
        
          
            
              
                
                  Doms, T.,
                
              
            
          
        
          
            
              
                
                  Vogt, T.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Nessler, B.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2103.16910.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Artificial Intelligence is one of the fastest growing technologies of the 21st century and accompanies us in our daily lives when interacting with technical applications. However, reliance on such technical systems is crucial for their widespread applicability and acceptance. The societal tools to express reliance are usually formalized by lawful regulations, i.e., standards, norms, accreditations, and certificates. Therefore, the TÜV AUSTRIA Group in cooperation with the Institute for Machine Learning at the Johannes Kepler University Linz, proposes a certification process and an audit catalog for Machine Learning applications. We are convinced that our approach can serve as the foundation for the certification of applications that use Machine Learning and Deep Learning, the techniques that drive the current revolution in Artificial Intelligence. While certain high-risk areas, such as fully autonomous robots in workspaces shared with humans, are still some time away from certification, we aim to cover low-risk applications with our certification procedure. Our holistic approach attempts to analyze Machine Learning applications from multiple perspectives to evaluate and verify the aspects of secure software development, functional requirements, data quality, data protection, and ethics. Inspired by existing work, we introduce four criticality levels to map the criticality of a Machine Learning application regarding the impact of its decisions on people, environment, and organizations. Currently, the audit catalog can be applied to low-risk applications within the scope of supervised learning as commonly encountered in industry. Guided by field experience, scientific developments, and market demands, the audit catalog will be extended and modified accordingly.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">QSAR2021</abbr>
    
  
  </div>

  <div id="schimunekhERGinterpretability2021" class="col-sm-8">
    
      <div class="title">Comparative assessment of interpretability methods of deep activity models for hERG</div>
      <div class="author">
        
          
            
              
                
                  Schimunek, J.,
                
              
            
          
        
          
            
              
                
                  Friedrich, L.,
                
              
            
          
        
          
            
              
                
                  Kuhn, D.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  Rippmann, F.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://www.ascctox.org/qsar2021" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Since many highly accurate predictive models for bioactivity and toxicity assays are based on Deep Learning methods, there has been a recent surge of interest in interpretability methods for Deep Learning approaches in drug discovery [1,2]. Interpretability methods are highly desired by human experts to enable them to make design decisions on the molecule based on the activity model. However, it is still unclear which of those interpretability methods are better identifying relevant substructures of molecules.  A method comparison is further complicated by the lack of ground truth and appropriate metrics. Here, we present the first comparative study of a set of interpretability methods for Deep Learning models for hERG inhibition. In our work, we compared layer-wise relevance propagation, feature gradients, saliency maps, integrated gradients, occlusion and Shapley values. In the quantitative analysis, known substructures which indicate hERG activity are used as ground truth [3]. Interpretability methods were compared by their ability to rank atoms, which are part of indicative substructures, first. The significantly best performing method is Shapley values with an area under-ROC-curve (AUC) of  0.74 ± 0.12, but also runner-up methods, such as Integrated Gradients, achieved similar results. The results indicate that interpretability methods for deep activity models have the potential to identify new toxicophores.

[1] Jiménez-Luna, J., et al. (2020). Nature Machine Intelligence, 2(10), 573-584.
[2]  Preuer, K., et al. (2019). In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning (pp. 331-345).
[3] Czodrowski, P. (2013). Journal of chemical information and modeling, 53, 2240–2251.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">QSAR</abbr>
    
  
  </div>

  <div id="seidlBenchmarking2021" class="col-sm-8">
    
      <div class="title">Benchmarking recent Deep Learning methods on the extended Tox21 data set</div>
      <div class="author">
        
          
            
              
                
                  Seidl, P.,
                
              
            
          
        
          
            
              
                
                  Halmich, C.,
                
              
            
          
        
          
            
              
                
                  Mayr, A.,
                
              
            
          
        
          
            
              
                
                  Vall, A.,
                
              
            
          
        
          
            
              
                
                  Ruch, P.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In </em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://www.ascctox.org/sites/default/files/QSAR%202021%20Program.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The Tox21 data set has evolved into a standard benchmark for computational QSAR methods in toxicology. One limitation of the Tox21 data set is, however, that it only contains twelve toxic assays which strongly restricts its power to distinguish the strength of computational methods. We ameliorate this problem by benchmarking on the extended Tox21 dataset with 68 publicly available assays in order to allow for a better assessment and characterization. The broader range of assays also allows for multi-task approaches, which have been particularly successful as predictive models. Furthermore, previous publications comparing methods on Tox21 did not include recent developments in the field of machine learning, such as graph neural and modern Hopfield networks. Thus we benchmark a set of prominent machine learning methods including those new types of neural networks. The results of the benchmarking study show that the best methods are modern Hopfield networks and multi-task graph neural networks with an average area-under-ROCcurve of 0.91 ± 0.05 (standard deviation across assays), while traditional methods, such as Random Forests fall behind by a substantial margin. Our results of the full benchmark suggest that multi-task learning has a stronger effect on the predictive performance than the choice of the representation of the molecules, such as graph, descriptors, or fingerprints.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">WRR</abbr>
    
  
  </div>

  <div id="nearing2021opinion" class="col-sm-8">
    
      <div class="title">What Role Does Hydrological Science Play in the Age of Machine Learning?</div>
      <div class="author">
        
          
            
              
                
                  Nearing, G.,
                
              
            
          
        
          
            
              
                
                  Kratzert, F.,
                
              
            
          
        
          
            
              
                
                  Sampson, A.,
                
              
            
          
        
          
            
              
                
                  Pelissier, C.,
                
              
            
          
        
          
            
              
                
                  Klotz, D.,
                
              
            
          
        
          
            
              
                
                  Frame, J.,
                
              
            
          
        
          
            
              
                
                  Prieto, C.,
                
              
            
          
        
          
            
              
                
                  and Gupta, H.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Water Resources Research</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020WR028091" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper is derived from a keynote talk given at the Google’s 2020 Flood Forecasting Meets Machine Learning Workshop. Recent experiments applying deep learning to rainfall-runoff simulation indicate that there is significantly more information in large-scale hydrological data sets than hydrologists have been able to translate into theory or models. While there is a growing interest in machine learning in the hydrological sciences community, in many ways, our community still holds deeply subjective and nonevidence-based preferences for models based on a certain type of “process understanding” that has historically not translated into accurate theory, models, or predictions. This commentary is a call to action for the hydrology community to focus on developing a quantitative understanding of where and when hydrological process understanding is valuable in a modeling discipline increasingly dominated by machine learning. We offer some potential perspectives and preliminary examples about how this might be accomplished.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  </div>

  <div id="ramsauerHopfieldNetworksAll2020" class="col-sm-8">
    
      <div class="title">Hopfield Networks Is All You Need</div>
      <div class="author">
        
          
            
              
                
                  Ramsauer, H.,
                
              
            
          
        
          
            
              
                
                  Schäfl, B.,
                
              
            
          
        
          
            
              
                
                  Lehner, J.,
                
              
            
          
        
          
            
              
                
                  Seidl, P.,
                
              
            
          
        
          
            
              
                
                  Widrich, M.,
                
              
            
          
        
          
            
              
                
                  Gruber, L.,
                
              
            
          
        
          
            
              
                
                  Holzleitner, M.,
                
              
            
          
        
          
            
              
                
                  Pavlović, M.,
                
              
            
          
        
          
            
              
                
                  Sandve, G.,
                
              
            
          
        
          
            
              
                
                  Greiff, V.,
                
              
            
          
        
          
            
              
                
                  Kreil, D.,
                
              
            
          
        
          
            
              
                
                  Kopp, M.,
                
              
            
          
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  Brandstetter, J.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="http://arxiv.org/abs/2008.02217" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
      <a href="https://ml-jku.github.io/hopfield-layers/" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
    
    
      <a href="https://github.com/ml-jku/hopfield-layers" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called "Hopfield", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: https://github.com/ml-jku/hopfield-layers</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="adler2020cross" class="col-sm-8">
    
      <div class="title">Cross-Domain Few-Shot Learning by Representation Fusion</div>
      <div class="author">
        
          
            
              
                
                  Adler, T.,
                
              
            
          
        
          
            
              
                
                  Brandstetter, J.,
                
              
            
          
        
          
            
              
                
                  Widrich, M.,
                
              
            
          
        
          
            
              
                
                  Mayr, A.,
                
              
            
          
        
          
            
              
                
                  Kreil, D.,
                
              
            
          
        
          
            
              
                
                  Kopp, M.,
                
              
            
          
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2010.06498</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2010.06498" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
      <a href="https://ml-jku.github.io/chef/" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
    
    
      <a href="https://github.com/ml-jku/chef" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In order to quickly adapt to new data, few-shot learning aims at learning from few examples, often by using already acquired knowledge. The new data often differs from the previously seen data due to a domain shift, that is, a change of the input-target distribution. While several methods perform well on small domain shifts like new target classes with similar inputs, larger domain shifts are still challenging. Large domain shifts may result in high-level concepts that are not shared between the original and the new domain. However, low-level concepts like edges in images might still be shared and useful. For cross-domain few-shot learning, we suggest representation fusion to unify different abstraction levels of a deep neural network into one representation. We propose Cross-domain Hebbian Ensemble Few-shot learning (CHEF), which achieves representation fusion by an ensemble of Hebbian learners acting on different layers of a deep neural network that was trained on the original domain. On the few-shot datasets miniImagenet and tieredImagenet, where the domain shift is small, CHEF is competitive with state-of-the-art methods. On cross-domain few-shot benchmark challenges with larger domain shifts, CHEF establishes novel state-of-the-art results in all categories. We further apply CHEF on a real-world cross-domain application in drug discovery. We consider a domain shift from bioactive molecules to environmental chemicals and drugs with twelve associated toxicity prediction tasks. On these tasks, that are highly relevant for computational drug discovery, CHEF significantly outperforms all its competitors.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="holzleitner2020convergence" class="col-sm-8">
    
      <div class="title">Convergence Proof for Actor-Critic Methods Applied to PPO and RUDDER</div>
      <div class="author">
        
          
            
              
                
                  Holzleitner, M.,
                
              
            
          
        
          
            
              
                
                  Gruber, L.,
                
              
            
          
        
          
            
              
                
                  Arjona-Medina, J.,
                
              
            
          
        
          
            
              
                
                  Brandstetter, J.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2020
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div>

  <div id="widrich2020modern" class="col-sm-8">
    
      <div class="title">Modern Hopfield networks and attention for immune repertoire classification</div>
      <div class="author">
        
          
            
              
                
                  Widrich, M.,
                
              
            
          
        
          
            
              
                
                  Schäfl, B.,
                
              
            
          
        
          
            
              
                
                  Ramsauer, H.,
                
              
            
          
        
          
            
              
                
                  Pavlović, M.,
                
              
            
          
        
          
            
              
                
                  Gruber, L.,
                
              
            
          
        
          
            
              
                
                  Holzleitner, M.,
                
              
            
          
        
          
            
              
                
                  Brandstetter, J.,
                
              
            
          
        
          
            
              
                
                  Sandve, G.,
                
              
            
          
        
          
            
              
                
                  Greiff, V.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://proceedings.neurips.cc/paper/2020/file/da4902cb0bc38210839714ebdcf0efc3-Paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/DeepRC" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>A central mechanism in machine learning is to identify, store, and recognize patterns. How to learn, access, and retrieve such patterns is crucial in Hopfield networks and the more recent transformer architectures. We show that the attention mechanism of transformer architectures is actually the update rule of modern Hopfield networks that can store exponentially many patterns. We exploit this high storage capacity of modern Hopfield networks to solve a challenging multiple instance learning (MIL) problem in computational biology: immune repertoire classification. In immune repertoire classification, a vast number of immune receptors are used to predict the immune status of an individual. This constitutes a MIL problem with an unprecedentedly massive number of instances, two orders of magnitude larger than currently considered problems, and with an extremely low witness rate. Accurate and interpretable machine learning methods solving this problem could pave the way towards new vaccines and therapies, which is currently a very relevant research topic intensified by the COVID-19 crisis. In this work, we present our novel method DeepRC that integrates transformer-like attention, or equivalently modern Hopfield networks, into deep learning architectures for massive MIL such as immune repertoire classification. We demonstrate that DeepRC outperforms all other methods with respect to predictive performance on large-scale experiments including simulated and real-world virus infection data and enables the extraction of sequence motifs that are connected to a given disease class. Source code and datasets: https://github.com/ml-jku/DeepRC</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="hofmarcherLargescaleLigandbasedVirtual2020" class="col-sm-8">
    
      <div class="title">Large-Scale Ligand-Based Virtual Screening for SARS-CoV-2 Inhibitors Using Deep Neural Networks</div>
      <div class="author">
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  Mayr, A.,
                
              
            
          
        
          
            
              
                
                  Rumetshofer, E.,
                
              
            
          
        
          
            
              
                
                  Ruch, P.,
                
              
            
          
        
          
            
              
                
                  Renz, P.,
                
              
            
          
        
          
            
              
                
                  Schimunek, J.,
                
              
            
          
        
          
            
              
                
                  Seidl, P.,
                
              
            
          
        
          
            
              
                
                  Vall, A.,
                
              
            
          
        
          
            
              
                
                  Widrich, M.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="http://arxiv.org/abs/2004.00979" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Due to the current severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pandemic, there is an urgent need for novel therapies and drugs. We conducted a large-scale virtual screening for small molecules that are potential CoV-2 inhibitors. To this end, we utilized "ChemAI", a deep neural network trained on more than 220M data points across 3.6M molecules from three public drug-discovery databases. With ChemAI, we screened and ranked one billion molecules from the ZINC database for favourable effects against CoV-2. We then reduced the result to the 30,000 top-ranked compounds, which are readily accessible and purchasable via the ZINC database. Additionally, we screened the DrugBank using ChemAI to allow for drug repurposing, which would be a fast way towards a therapy. We provide these top-ranked compounds of ZINC and DrugBank as a library for further screening with bioassays at https://github.com/ml-jku/sars-cov-inhibitors-chemai.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="renzFailureModesMolecule2020a" class="col-sm-8">
    
      <div class="title">On Failure Modes in Molecule Generation and Optimization</div>
      <div class="author">
        
          
            
              
                
                  Renz, P.,
                
              
            
          
        
          
            
              
                
                  Van Rompaey, D.,
                
              
            
          
        
          
            
              
                
                  Wegner, J.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1016/j.ddtec.2020.09.003" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
      <a href="https://github.com/ml-jku/mgenerators-failure-modes" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>There has been a wave of generative models for molecules triggered by advances in the field of Deep Learning. These generative models are often used to optimize chemical compounds towards particular properties or a desired biological activity. The evaluation of generative models remains challenging and suggested performance metrics or scoring functions often do not cover all relevant aspects of drug design projects. In this work, we highlight some unintended failure modes in molecular generation and optimization and how these evade detection by current performance metrics.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Springer</abbr>
    
  
  </div>

  <div id="sturmIndustryscaleApplicationEvaluation2020" class="col-sm-8">
    
      <div class="title">Industry-Scale Application and Evaluation of Deep Learning for Drug Target Prediction</div>
      <div class="author">
        
          
            
              
                
                  Sturm, N.,
                
              
            
          
        
          
            
              
                
                  Mayr, A.,
                
              
            
          
        
          
            
              
                
                  Le Van, T.,
                
              
            
          
        
          
            
              
                
                  Chupakhin, V.,
                
              
            
          
        
          
            
              
                
                  Ceulemans, H.,
                
              
            
          
        
          
            
              
                
                  Wegner, J.,
                
              
            
          
        
          
            
              
                
                  Golib-Dzib, J.,
                
              
            
          
        
          
            
              
                
                  Jeliazkova, N.,
                
              
            
          
        
          
            
              
                
                  Vandriessche, Y.,
                
              
            
          
        
          
            
              
                
                  Böhm, S.,
                
              
            
          
        
          
            
              
                
                  Cima, V.,
                
              
            
          
        
          
            
              
                
                  Martinovic, J.,
                
              
            
          
        
          
            
              
                
                  Greene, N.,
                
              
            
          
        
          
            
              
                
                  Vander Aa, T.,
                
              
            
          
        
          
            
              
                
                  Ashby, T.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  Engkvist, O.,
                
              
            
          
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  and Chen, H.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1186/s13321-020-00428-5" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Artificial intelligence (AI) is undergoing a revolution thanks to the breakthroughs of machine learning algorithms in computer vision, speech recognition, natural language processing and generative modelling. Recent works on publicly available pharmaceutical data showed that AI methods are highly promising for Drug Target prediction. However, the quality of public data might be different than that of industry data due to different labs reporting measurements, different measurement techniques, fewer samples and less diverse and specialized assays. As part of a European funded project (ExCAPE), that brought together expertise from pharmaceutical industry, machine learning, and high-performance computing, we investigated how well machine learning models obtained from public data can be transferred to internal pharmaceutical industry data. Our results show that machine learning models trained on public data can indeed maintain their predictive power to a large degree when applied to industry data. Moreover, we observed that deep learning derived machine learning models outperformed comparable models, which were trained by other machine learning algorithms, when applied to internal pharmaceutical company datasets. To our knowledge, this is the first large-scale study evaluating the potential of machine learning and especially deep learning directly at the level of industry-scale settings and moreover investigating the transferability of publicly learned target prediction models towards industrial bioactivity prediction pipelines.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">bioarXiv</abbr>
    
  
  </div>

  <div id="widrichDeepRCImmuneRepertoire2020" class="col-sm-8">
    
      <div class="title">DeepRC: Immune Repertoire Classification with Attention-Based Deep Massive Multiple Instance Learning</div>
      <div class="author">
        
          
            
              
                
                  Widrich, M.,
                
              
            
          
        
          
            
              
                
                  Schäfl, B.,
                
              
            
          
        
          
            
              
                
                  Pavlović, M.,
                
              
            
          
        
          
            
              
                
                  Sandve, G.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  Greiff, V.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1101/2020.04.12.038158" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p><h3>Abstract</h3> <p>High-throughput immunosequencing allows reconstructing the immune repertoire of an individual, which is a unique opportunity for new immunotherapies, immunodiagnostics, and vaccine design. Since immune repertoires are shaped by past and current immune events, such as infection and disease, and thus record an individual’s state of health, immune repertoire sequencing data may enable the prediction of health and disease using machine learning. However, finding the connections between an individual’s repertoire and the individual’s disease class, with potentially hundreds of thousands to millions of short sequences per individual, poses a difficult and unique challenge for machine learning methods. In this work, we present our method DeepRC that combines a Deep Learning architecture with attentionbased multiple instance learning. To validate that DeepRC accurately predicts an individual’s disease class based on its immune repertoire and determines the associated class-specific sequence motifs, we applied DeepRC in four large-scale experiments encompassing ground-truth simulated as well as real-world virus infection data. We demonstrate that DeepRC outperforms all tested methods with respect to predictive performance and enables the extraction of those sequence motifs that are connected to a given disease class.</p></p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kimeswenger2020ArtificialNeuralNetworks" class="col-sm-8">
    
      <div class="title">Artificial Neural Networks and Pathologists Recognize Basal Cell Carcinomas Based on Different Histological Patterns</div>
      <div class="author">
        
          
            
              
                
                  Kimeswenger, S.,
                
              
            
          
        
          
            
              
                
                  Tschandl, P.,
                
              
            
          
        
          
            
              
                
                  Noack, P.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  Rumetshofer, E.,
                
              
            
          
        
          
            
              
                
                  Kindermann, H.,
                
              
            
          
        
          
            
              
                
                  Silye, R.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  Kaltenbrunner, M.,
                
              
            
          
        
          
            
              
                
                  Guenova, E.,
                
              
            
          
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  and Hoetzenecker, W.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://www.nature.com/articles/s41379-020-00712-7" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent advances in artificial intelligence, particularly in the field of deep learning, have enabled researchers to create compelling algorithms for medical image analysis. Histological slides of basal cell carcinomas (BCCs), the most frequent skin tumor, are accessed by pathologists on a daily basis and are therefore well suited for automated prescreening by neural networks for the identification of cancerous regions and swift tumor classification.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div>

  <div id="Arjona:19" class="col-sm-8">
    
      <div class="title">RUDDER: Return Decomposition for Delayed Rewards</div>
      <div class="author">
        
          
            
              
                
                  Arjona-Medina, J.,
                
              
            
          
        
          
            
              
                
                  Gillhofer, M.,
                
              
            
          
        
          
            
              
                
                  Widrich, M.,
                
              
            
          
        
          
            
              
                
                  Unterthiner, T.,
                
              
            
          
        
          
            
              
                
                  Brandstetter, J.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://papers.nips.cc/paper/2019/file/16105fb9cc614fc29e1bda00dab60d41-Paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
      <a href="https://ml-jku.github.io/rudder/" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
    
    
      <a href="https://github.com/ml-jku/rudder" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose RUDDER, a novel reinforcement learning approach for delayed rewards in finite Markov decision processes (MDPs). In MDPs the Q-values are equal to the expected immediate reward plus the expected future rewards. The latter are related to bias problems in temporal difference (TD) learning and to high variance problems in Monte Carlo (MC) learning. Both problems are even more severe when rewards are delayed. RUDDER aims at making the expected future rewards zero, which simplifies Q-value estimation to computing the mean of the immediate reward. We propose the following two new concepts to push the expected future rewards toward zero. (i) Reward redistribution that leads to return-equivalent decision processes with the same optimal policies and, when optimal, zero expected future rewards. (ii) Return decomposition via contribution analysis which transforms the reinforcement learning task into a regression task at which deep learning excels. On artificial tasks with delayed rewards, RUDDER is significantly faster than MC and exponentially faster than Monte Carlo Tree Search (MCTS), TD({λ}), and reward shaping approaches. At Atari games, RUDDER on top of a Proximal Policy Optimization (PPO) baseline improves the scores, which is most prominent at games with delayed rewards. Source code is available at \url{https://github.com/ml-jku/rudder} and demonstration videos at \url{https://goo.gl/EQerZV}.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="arrasExplainingInterpretingLSTMs2019" class="col-sm-8">
    
      <div class="title">Explaining and Interpreting LSTMs</div>
      <div class="author">
        
          
            
              
                
                  Arras, L.,
                
              
            
          
        
          
            
              
                
                  Arjona-Medina, J.,
                
              
            
          
        
          
            
              
                
                  Widrich, M.,
                
              
            
          
        
          
            
              
                
                  Montavon, G.,
                
              
            
          
        
          
            
              
                
                  Gillhofer, M.,
                
              
            
          
        
          
            
              
                
                  Müller, K.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Samek, W.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1007/978-3-030-28954-6_11" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While neural networks have acted as a strong unifying force in the design of modern AI systems, the neural network architectures themselves remain highly heterogeneous due to the variety of tasks to be solved. In this chapter, we explore how to adapt the Layer-wise Relevance Propagation (LRP) technique used for explaining the predictions of feed-forward networks to the LSTM architecture used for sequential data modeling and forecasting. The special accumulators and gated interactions present in the LSTM require both a new propagation scheme and an extension of the underlying theoretical framework to deliver faithful explanations.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Springer</abbr>
    
  
  </div>

  <div id="hofmarcherVisualSceneUnderstanding2019" class="col-sm-8">
    
      <div class="title">Visual Scene Understanding for Autonomous Driving Using Semantic Segmentation</div>
      <div class="author">
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  Unterthiner, T.,
                
              
            
          
        
          
            
              
                
                  Arjona-Medina, J.,
                
              
            
          
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Nessler, B.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1007/978-3-030-28954-6_15" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Deep neural networks are an increasingly important technique for autonomous driving, especially as a visual perception component. Deployment in a real environment necessitates the explainability and inspectability of the algorithms controlling the vehicle. Such insightful explanations are relevant not only for legal issues and insurance matters but also for engineers and developers in order to achieve provable functional quality guarantees. This applies to all scenarios where the results of deep networks control potentially life threatening machines. We suggest the use of a tiered approach, whose main component is a semantic segmentation model, over an end-to-end approach for an autonomous driving system. In order for a system to provide meaningful explanations for its decisions it is necessary to give an explanation about the semantics that it attributes to the complex sensory inputs that it perceives. In the context of high-dimensional visual input this attribution is done as a pixel-wise classification process that assigns an object class to every pixel in the image. This process is called semantic segmentation.We propose an architecture that delivers real-time viable segmentation performance and which conforms to the limitations in computational power that is available in production vehicles. The output of such a semantic segmentation model can be used as an input for an interpretable autonomous driving system.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kimeswengerDetectingCutaneousBasal2019" class="col-sm-8">
    
      <div class="title">Detecting Cutaneous Basal Cell Carcinomas in Ultra-High Resolution and Weakly Labelled Histopathological Images</div>
      <div class="author">
        
          
            
              
                
                  Kimeswenger, S.,
                
              
            
          
        
          
            
              
                
                  Rumetshofer, E.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  Tschandl, P.,
                
              
            
          
        
          
            
              
                
                  Kittler, H.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  Hötzenecker, W.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="http://arxiv.org/abs/1911.06616" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/1911.06616.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Diagnosing basal cell carcinomas (BCC), one of the most common cutaneous malignancies in humans, is a task regularly performed by pathologists and dermato-pathologists. Improving histological diagnosis by providing diagnosis suggestions, i.e. computer-assisted diagnoses is actively researched to improve safety, quality and efficiency. Increasingly, machine learning methods are applied due to their superior performance. However, typical images obtained by scanning histological sections often have a resolution that is prohibitive for processing with current state-of-the-art neural networks. Furthermore, the data pose a problem of weak labels, since only a tiny fraction of the image is indicative of the disease class, whereas a large fraction of the image is highly similar to the non-disease class. The aim of this study is to evaluate whether it is possible to detect basal cell carcinomas in histological sections using attention-based deep learning models and to overcome the ultra-high resolution and the weak labels of whole slide images. We demonstrate that attention-based models can indeed yield almost perfect classification performance with an AUC of 0.99.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="lehner2019patch" class="col-sm-8">
    
      <div class="title">Patch Refinement - Localized 3D Object Detection</div>
      <div class="author">
        
          
            
              
                
                  Lehner, J.,
                
              
            
          
        
          
            
              
                
                  Mitterecker, A.,
                
              
            
          
        
          
            
              
                
                  Adler, T.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  Nessler, B.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/1910.04093" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/1910.04093" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We introduce Patch Refinement a two-stage model for accurate 3D object detection and localization from point cloud data. Patch Refinement is composed of two independently trained Voxelnet-based networks, a Region Proposal Network (RPN) and a Local Refinement Network (LRN). We decompose the detection task into a preliminary Bird’s Eye View (BEV) detection step and a local 3D detection step. Based on the proposed BEV locations by the RPN, we extract small point cloud subsets ("patches"), which are then processed by the LRN, which is less limited by memory constraints due to the small area of each patch. Therefore, we can apply encoding with a higher voxel resolution locally. The independence of the LRN enables the use of additional augmentation techniques and allows for an efficient, regression focused training as it uses only a small fraction of each scene. Evaluated on the KITTI 3D object detection benchmark, our submission from January 28, 2019, outperformed all previous entries on all three difficulties of the class car, using only 50 % of the available training data and only LiDAR information.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="klambauerMachineLearningDrug2019" class="col-sm-8">
    
      <div class="title">Machine Learning in Drug Discovery</div>
      <div class="author">
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Rarey, M.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
    
    
      <a href="https://doi.org/10.1021/acs.jcim.9b00136" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">WRR</abbr>
    
  
  </div>

  <div id="kratzertImprovedPredictionsUngauged2019" class="col-sm-8">
    
      <div class="title">Toward Improved Predictions in Ungauged Basins: Exploiting the Power of Machine Learning</div>
      <div class="author">
        
          
            
              
                
                  Kratzert, F.,
                
              
            
          
        
          
            
              
                
                  Klotz, D.,
                
              
            
          
        
          
            
              
                
                  Herrnegger, M.,
                
              
            
          
        
          
            
              
                
                  Sampson, A.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Nearing, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1029/2019WR026065" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Long short-term memory (LSTM) networks offer unprecedented accuracy for prediction in ungauged basins. We trained and tested several LSTMs on 531 basins from the CAMELS data set using k-fold validation, so that predictions were made in basins that supplied no training data. The training and test data set included ∼30 years of daily rainfall-runoff data from catchments in the United States ranging in size from 4 to 2,000 km2 with aridity index from 0.22 to 5.20, and including 12 of the 13 IGPB vegetated land cover classifications. This effectively “ungauged” model was benchmarked over a 15-year validation period against the Sacramento Soil Moisture Accounting (SAC-SMA) model and also against the NOAA National Water Model reanalysis. SAC-SMA was calibrated separately for each basin using 15 years of daily data. The out-of-sample LSTM had higher median Nash-Sutcliffe Efficiencies across the 531 basins (0.69) than either the calibrated SAC-SMA (0.64) or the National Water Model (0.58). This indicates that there is (typically) sufficient information in available catchment attributes data about similarities and differences between catchment-level rainfall-runoff behaviors to provide out-of-sample simulations that are generally more accurate than current models under ideal (i.e., calibrated) conditions. We found evidence that adding physical constraints to the LSTM models might improve simulations, which we suggest motivates future research related to physics-guided machine learning.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">HESS</abbr>
    
  
  </div>

  <div id="kratzertLearningUniversalRegional2019" class="col-sm-8">
    
      <div class="title">Towards Learning Universal, Regional, and Local Hydrological Behaviors via Machine Learning Applied to Large-Sample Datasets</div>
      <div class="author">
        
          
            
              
                
                  Kratzert, F.,
                
              
            
          
        
          
            
              
                
                  Klotz, D.,
                
              
            
          
        
          
            
              
                
                  Shalev, G.,
                
              
            
          
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Nearing, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.5194/hess-23-5089-2019" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p><p><strong>Abstract.</strong> Regional rainfall–runoff modeling is an old but still mostly outstanding problem in the hydrological sciences. The problem currently is that traditional hydrological models degrade significantly in performance when calibrated for multiple basins together instead of for a single basin alone. In this paper, we propose a novel, data-driven approach using Long Short-Term Memory networks (LSTMs) and demonstrate that under a “big data” paradigm, this is not necessarily the case. By training a single LSTM model on 531 basins from the CAMELS dataset using meteorological time series data and static catchment attributes, we were able to significantly improve performance compared to a set of several different hydrological benchmark models. Our proposed approach not only significantly outperforms hydrological models that were calibrated regionally, but also achieves better performance than hydrological models that were calibrated for each basin individually. Furthermore, we propose an adaption to the standard LSTM architecture, which we call an Entity-Aware-LSTM (EA-LSTM), that allows for learning catchment similarities as a feature layer in a deep learning model. We show that these learned catchment similarities correspond well to what we would expect from prior hydrological understanding.</p></p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Springer</abbr>
    
  
  </div>

  <div id="kratzertNeuralHydrologyInterpretingLSTMs2019" class="col-sm-8">
    
      <div class="title">NeuralHydrology – Interpreting LSTMs in Hydrology</div>
      <div class="author">
        
          
            
              
                
                  Kratzert, F.,
                
              
            
          
        
          
            
              
                
                  Herrnegger, M.,
                
              
            
          
        
          
            
              
                
                  Klotz, D.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1007/978-3-030-28954-6_19" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Despite the huge success of Long Short-Term Memory networks, their applications in environmental sciences are scarce. We argue that one reason is the difficulty to interpret the internals of trained networks. In this study, we look at the application of LSTMs for rainfall-runoff forecasting, one of the central tasks in the field of hydrology, in which the river discharge has to be predicted from meteorological observations. LSTMs are particularly well-suited for this problem since memory cells can represent dynamic reservoirs and storages, which are essential components in state-space modelling approaches of the hydrological system. On basis of two different catchments, one with snow influence and one without, we demonstrate how the trained model can be analyzed and interpreted. In the process, we show that the network internally learns to represent patterns that are consistent with our qualitative understanding of the hydrological system.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="mendenCommunityAssessmentAdvance2019" class="col-sm-8">
    
      <div class="title">Community Assessment to Advance Computational Prediction of Cancer Drug Combinations in a Pharmacogenomic Screen</div>
      <div class="author">
        
          
            
              
                
                  Menden, M.,
                
              
            
          
        
          
            
              
                
                  Wang, D.,
                
              
            
          
        
          
            
              
                
                  Mason, M.,
                
              
            
          
        
          
            
              
                
                  Szalai, B.,
                
              
            
          
        
          
            
              
                
                  Bulusu, K.,
                
              
            
          
        
          
            
              
                
                  Guan, Y.,
                
              
            
          
        
          
            
              
                
                  Yu, T.,
                
              
            
          
        
          
            
              
                
                  Kang, J.,
                
              
            
          
        
          
            
              
                
                  Jeon, M.,
                
              
            
          
        
          
            
              
                
                  Wolfinger, R.,
                
              
            
          
        
          
            
              
                
                  Nguyen, T.,
                
              
            
          
        
          
            
              
                
                  Zaslavskiy, M.,
                
              
            
          
        
          
            
              
                
                  Jang, I.,
                
              
            
          
        
          
            
              
                
                  Ghazoui, Z.,
                
              
            
          
        
          
            
              
                
                  Ahsen, M.,
                
              
            
          
        
          
            
              
                
                  Vogel, R.,
                
              
            
          
        
          
            
              
                
                  Neto, E.,
                
              
            
          
        
          
            
              
                
                  Norman, T.,
                
              
            
          
        
          
            
              
                
                  Tang, E.,
                
              
            
          
        
          
            
              
                
                  Garnett, M.,
                
              
            
          
        
          
            
              
                
                  Veroli, G.,
                
              
            
          
        
          
            
              
                
                  Fawell, S.,
                
              
            
          
        
          
            
              
                
                  Stolovitzky, G.,
                
              
            
          
        
          
            
              
                
                  Guinney, J.,
                
              
            
          
        
          
            
              
                
                  Dry, J.,
                
              
            
          
        
          
            
              
                
                  and Saez-Rodriguez, J.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1038/s41467-019-09799-2" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The effectiveness of most cancer targeted therapies is short-lived. Tumors often develop resistance that might be overcome with drug combinations. However, the number of possible combinations is vast, necessitating data-driven approaches to find optimal patient-specific treatments. Here we report AstraZeneca’s large drug combination dataset, consisting of 11,576 experiments from 910 combinations across 85 molecularly characterized cancer cell lines, and results of a DREAM Challenge to evaluate computational strategies for predicting synergistic drug pairs and biomarkers. 160 teams participated to provide a comprehensive methodological development and benchmarking. Winning methods incorporate prior knowledge of drug-target interactions. Synergy is predicted with an accuracy matching biological replicates for &gt;60% of combinations. However, 20% of drug combinations are poorly predicted by all methods. Genomic rationale for synergy predictions are identified, including ADAM17 inhibitor antagonism when combined with PIK3CB/D inhibition contrasting to synergy when combined with other PI3K-pathway inhibitors in PIK3CA mutant cells.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="preuerInterpretableDeepLearning2019" class="col-sm-8">
    
      <div class="title">Interpretable Deep Learning in Drug Discovery</div>
      <div class="author">
        
          
            
              
                
                  Preuer, K.,
                
              
            
          
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  Rippmann, F.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Unterthiner, T.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1007/978-3-030-28954-6_18" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Without any means of interpretation, neural networks that predict molecular properties and bioactivities are merely black boxes. We will unravel these black boxes and will demonstrate approaches to understand the learned representations which are hidden inside these models. We show how single neurons can be interpreted as classifiers which determine the presence or absence of pharmacophore- or toxicophore-like structures, thereby generating new insights and relevant knowledge for chemistry, pharmacology and biochemistry. We further discuss how these novel pharmacophores/toxicophores can be determined from the network by identifying the most relevant components of a compound for the prediction of the network. Additionally, we propose a method which can be used to extract new pharmacophores from a model and will show that these extracted structures are consistent with literature findings. We envision that having access to such interpretable knowledge is a crucial aid in the development and design of new pharmaceutically active molecules, and helps to investigate and understand failures and successes of current methods.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div>

  <div id="renzUncertaintyEstimationMethods2019" class="col-sm-8">
    
      <div class="title">Uncertainty Estimation Methods to Support Decision-Making in Early Phases of Drug Discovery</div>
      <div class="author">
        
          
            
              
                
                  Renz, P.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>It takes about a decade to develop a new drug by a process in which a large number of decisions have to be made. Those decisions are critical for the success or failure of a multi-million dollar drug discovery project, which could save many lives or increase life quality. Decisions in early phases of drug discovery, such as the selection of certain series of chemical compounds, are particularly impactful on the success rate. Machine learning models are increasingly used to inform the decision making process by predicting desired effects, undesired effects, such as toxicity, molecular properties, or which wet-lab test to perform next. Thus, accurately quantifying the uncertainties of the models’ outputs is critical, for example, in order to calculate expected utilities, to estimate the risk and the potential gain. In this work, we review, assess and compare recent uncertainty estimation methods with respect to their use in drug discovery projects. We test both, which methods give well calibrated prediction and which ones perform well at misclassification detection. For the latter, we find the entropy of the predictive distribution performs best. Finally, we discuss the problem of defining out-of-distribution samples for prediction tasks on chemical compounds.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div>

  <div id="lehner2019" class="col-sm-8">
    
      <div class="title">Patch Refinement – Localized 3D Object Detection</div>
      <div class="author">
        
          
            
              
                
                  Lehner, J.,
                
              
            
          
        
          
            
              
                
                  Mitterecker, A.,
                
              
            
          
        
          
            
              
                
                  Adler, T.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  Nessler, B.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Workshop on Machine Learning for Autonomous Driving, Neural Information Processing Systems (NeurIPS)</em>
      
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">JCIM</abbr>
    
  
  </div>

  <div id="hofmarcher2019" class="col-sm-8">
    
      <div class="title">Accurate Prediction of Biological Assays with High-Throughput Microscopy Images and Convolutional Networks</div>
      <div class="author">
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  Rumetshofer, E.,
                
              
            
          
        
          
            
              
                
                  Clevert, D.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1021/acs.jcim.8b00670" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Predicting the outcome of biological assays based on high-throughput imaging data is a highly promising task in drug discovery since it can tremendously increase hit rates and suggest novel chemical scaffolds. However, end-to-end learning with convolutional neural networks (CNNs) has not been assessed for the task biological assay prediction despite the success of these networks at visual recognition. We compared several CNNs trained directly on high-throughput imaging data to a) CNNs trained on cell-centric crops and to b) the current state-of-the-art: fully connected networks trained on precalculated morphological cell features. The comparison was performed on the Cell Painting data set, the largest publicly available data set of microscopic images of cells with approximately 30,000 compound treatments. We found that CNNs perform significantly better at predicting the outcome of assays than fully connected networks operating on precomputed morphological features of cells. Surprisingly, the best performing method could predict 32% of the 209 biological assays at high predictive performance (AUC &gt; 0.9) indicating that the cell morphology changes contain a large amount of information about compound activities. Our results suggest that many biological assays could be replaced by high-throughput imaging together with convolutional neural networks and that the costly cell segmentation and feature extraction step can be replaced by convolutional neural networks.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="unterthinerCoulombGANsProvably2018" class="col-sm-8">
    
      <div class="title">Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields</div>
      <div class="author">
        
          
            
              
                
                  Unterthiner, T.,
                
              
            
          
        
          
            
              
                
                  Nessler, B.,
                
              
            
          
        
          
            
              
                
                  Seward, C.,
                
              
            
          
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  Heusel, M.,
                
              
            
          
        
          
            
              
                
                  Ramsauer, H.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="http://arxiv.org/abs/1708.08819" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on a variety of image datasets. On LSUN and celebA, Coulomb GANs set a new state of the art and produce a previously unseen variety of different samples.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="hochreiterMachineLearningDrug2018" class="col-sm-8">
    
      <div class="title">Machine Learning in Drug Discovery</div>
      <div class="author">
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  and Rarey, M.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
    
    
      <a href="https://doi.org/10.1021/acs.jcim.8b00478" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="mayrLargescaleComparisonMachine2018" class="col-sm-8">
    
      <div class="title">Large-Scale Comparison of Machine Learning Methods for Drug Target Prediction on ChEMBL</div>
      <div class="author">
        
          
            
              
                
                  Mayr, A.,
                
              
            
          
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  Unterthiner, T.,
                
              
            
          
        
          
            
              
                
                  Steijaert, M.,
                
              
            
          
        
          
            
              
                
                  Wegner, J.,
                
              
            
          
        
          
            
              
                
                  Ceulemans, H.,
                
              
            
          
        
          
            
              
                
                  Clevert, D.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1039/C8SC00148K" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Deep learning is currently the most successful machine learning technique in a wide range of application areas and has recently been applied successfully in drug discovery research to predict potential drug targets and to screen for active molecules. However, due to (1) the lack of large-scale studies, (2) the compound series bias that is characteristic of drug discovery datasets and (3) the hyperparameter selection bias that comes with the high number of potential deep learning architectures, it remains unclear whether deep learning can indeed outperform existing computational methods in drug discovery tasks. We therefore assessed the performance of several deep learning methods on a large-scale drug discovery dataset and compared the results with those of other machine learning and target prediction methods. To avoid potential biases from hyperparameter selection or compound series, we used a nested cluster-cross-validation strategy. We found (1) that deep learning methods significantly outperform all competing methods and (2) that the predictive performance of deep learning is in many cases comparable to that of tests performed in wet labs (i.e., in vitro assays).</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="preuerDeepSynergyPredictingAnticancer2018" class="col-sm-8">
    
      <div class="title">DeepSynergy: Predicting Anti-Cancer Drug Synergy with Deep Learning</div>
      <div class="author">
        
          
            
              
                
                  Preuer, K.,
                
              
            
          
        
          
            
              
                
                  Lewis, R.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  Bender, A.,
                
              
            
          
        
          
            
              
                
                  Bulusu, K.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1093/bioinformatics/btx806" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While drug combination therapies are a well-established concept in cancer treatment, identifying novel synergistic combinations is challenging due to the size of combinatorial space. However, computational approaches have emerged as a time- and cost-efficient way to prioritize combinations to test, based on recently available large-scale combination screening data. Recently, Deep Learning has had an impact in many research areas by achieving new state-of-the-art model performance. However, Deep Learning has not yet been applied to drug synergy prediction, which is the approach we present here, termed DeepSynergy. DeepSynergy uses chemical and genomic information as input information, a normalization strategy to account for input data heterogeneity, and conical layers to model drug synergies.DeepSynergy was compared to other machine learning methods such as Gradient Boosting Machines, Random Forests, Support Vector Machines and Elastic Nets on the largest publicly available synergy dataset with respect to mean squared error. DeepSynergy significantly outperformed the other methods with an improvement of 7.2% over the second best method at the prediction of novel drug combinations within the space of explored drugs and cell lines. At this task, the mean Pearson correlation coefficient between the measured and the predicted values of DeepSynergy was 0.73. Applying DeepSynergy for classification of these novel drug combinations resulted in a high predictive performance of an AUC of 0.90. Furthermore, we found that all compared methods exhibit low predictive performance when extrapolating to unexplored drugs or cell lines, which we suggest is due to limitations in the size and diversity of the dataset. We envision that DeepSynergy could be a valuable tool for selecting novel synergistic drug combinations.DeepSynergy is available via www.bioinf.jku.at/software/DeepSynergy.Supplementary data are available at Bioinformatics online.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="preuerFrechetChemNetDistance2018" class="col-sm-8">
    
      <div class="title">Fréchet ChemNet Distance: A Metric for Generative Models for Molecules in Drug Discovery</div>
      <div class="author">
        
          
            
              
                
                  Preuer, K.,
                
              
            
          
        
          
            
              
                
                  Renz, P.,
                
              
            
          
        
          
            
              
                
                  Unterthiner, T.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1021/acs.jcim.8b00234" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The new wave of successful generative models in machine learning has increased the interest in deep learning driven de novo drug design. However, method comparison is difficult because of various flaws of the currently employed evaluation metrics. We propose an evaluation metric for generative models called Fréchet ChemNet distance (FCD). The advantage of the FCD over previous metrics is that it can detect whether generated molecules are diverse and have similar chemical and biological properties as real molecules.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  </div>

  <div id="rumetshoferHumanlevelProteinLocalization2018" class="col-sm-8">
    
      <div class="title">Human-Level Protein Localization with Convolutional Neural Networks</div>
      <div class="author">
        
          
            
              
                
                  Rumetshofer, E.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  Röhrl, C.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Klambauer, G.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In </em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://openreview.net/forum?id=ryl5khRcKm" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Localizing a specific protein in a human cell is essential for understanding cellular functions and biological processes of underlying diseases. A promising, low-cost,and time-efficient...</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="sewardFirstOrderGenerative2018" class="col-sm-8">
    
      <div class="title">First Order Generative Adversarial Networks</div>
      <div class="author">
        
          
            
              
                
                  Seward, C.,
                
              
            
          
        
          
            
              
                
                  Unterthiner, T.,
                
              
            
          
        
          
            
              
                
                  Bergmann, U.,
                
              
            
          
        
          
            
              
                
                  Jetchev, N.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="http://arxiv.org/abs/1802.04591" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/1802.04591.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>GANs excel at learning high dimensional distributions, but they can update generator parameters in directions that do not correspond to the steepest descent direction of the objective. Prominent examples of problematic update directions include those used in both Goodfellow’s original GAN and the WGAN-GP. To formally describe an optimal update direction, we introduce a theoretical framework which allows the derivation of requirements on both the divergence and corresponding method for determining an update direction, with these requirements guaranteeing unbiased mini-batch updates in the direction of steepest descent. We propose a novel divergence which approximates the Wasserstein distance while regularizing the critic’s first order information. Together with an accompanying update direction, this divergence fulfills the requirements for unbiased steepest descent updates. We verify our method, the First Order GAN, with image generation on CelebA, LSUN and CIFAR-10 and set a new state of the art on the One Billion Word language generation task. Code to reproduce experiments is available.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="steinwandterMultivariateAnalyticsChromatographic2018" class="col-sm-8">
    
      <div class="title">Multivariate Analytics of Chromatographic Data: Visual Computing Based on Moving Window Factor Models</div>
      <div class="author">
        
          
            
              
                
                  Steinwandter, V.,
                
              
            
          
        
          
            
              
                
                  Šišmiš, M.,
                
              
            
          
        
          
            
              
                
                  Sagmeister, P.,
                
              
            
          
        
          
            
              
                
                  Bodenhofer, U.,
                
              
            
          
        
          
            
              
                
                  and Herwig, C.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1016/j.jchromb.2018.06.010" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Chromatography is one of the most versatile unit operations in the biotechnological industry. Regulatory initiatives like Process Analytical Technology and Quality by Design led to the implementation of new chromatographic devices. Those represent an almost inexhaustible source of data. However, the analysis of large datasets is complicated, and significant amounts of information stay hidden in big data. Here we present a new, top-down approach for the systematic analysis of chromatographic datasets. It is the goal of this approach to analyze the dataset as a whole, starting with the most important, global information. The workflow should highlight interesting regions (outliers, drifts, data inconsistencies), and help to localize those regions within a multi-dimensional space in a straightforward way. Moving window factor models were used to extract the most important information, focusing on the differences between samples. The prototype was implemented as an interactive visualization tool for the explorative analysis of complex datasets. We found that the tool makes it convenient to localize variances in a multidimensional dataset and allows to differentiate between explainable and unexplainable variance. Starting with one global difference descriptor per sample, the analysis ends up with highly resolute temporally dependent difference descriptor values, thought as a starting point for the detailed analysis of the underlying raw data.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="fischerDefiningObjectiveClusters2018" class="col-sm-8">
    
      <div class="title">Defining Objective Clusters for Rabies Virus Sequences Using Affinity Propagation Clustering</div>
      <div class="author">
        
          
            
              
                
                  Fischer, S.,
                
              
            
          
        
          
            
              
                
                  Freuling, C.,
                
              
            
          
        
          
            
              
                
                  Müller, T.,
                
              
            
          
        
          
            
              
                
                  Pfaff, F.,
                
              
            
          
        
          
            
              
                
                  Bodenhofer, U.,
                
              
            
          
        
          
            
              
                
                  Höper, D.,
                
              
            
          
        
          
            
              
                
                  Fischer, M.,
                
              
            
          
        
          
            
              
                
                  Marston, D.,
                
              
            
          
        
          
            
              
                
                  Fooks, A.,
                
              
            
          
        
          
            
              
                
                  Mettenleiter, T.,
                
              
            
          
        
          
            
              
                
                  Conraths, F.,
                
              
            
          
        
          
            
              
                
                  and Homeier-Bachmann, T.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1371/journal.pntd.0006182" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Rabies is caused by lyssaviruses, and is one of the oldest known zoonoses. In recent years, more than 21,000 nucleotide sequences of rabies viruses (RABV), from the prototype species rabies lyssavirus, have been deposited in public databases. Subsequent phylogenetic analyses in combination with metadata suggest geographic distributions of RABV. However, these analyses somewhat experience technical difficulties in defining verifiable criteria for cluster allocations in phylogenetic trees inviting for a more rational approach. Therefore, we applied a relatively new mathematical clustering algorythm named ‘affinity propagation clustering’ (AP) to propose a standardized sub-species classification utilizing full-genome RABV sequences. Because AP has the advantage that it is computationally fast and works for any meaningful measure of similarity between data samples, it has previously been applied successfully in bioinformatics, for analysis of microarray and gene expression data, however, cluster analysis of sequences is still in its infancy. Existing (516) and original (46) full genome RABV sequences were used to demonstrate the application of AP for RABV clustering. On a global scale, AP proposed four clusters, i.e. New World cluster, Arctic/Arctic-like, Cosmopolitan, and Asian as previously assigned by phylogenetic studies. By combining AP with established phylogenetic analyses, it is possible to resolve phylogenetic relationships between verifiably determined clusters and sequences. This workflow will be useful in confirming cluster distributions in a uniform transparent manner, not only for RABV, but also for other comparative sequence analyses.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="clevertRectifiedFactorNetworks2017" class="col-sm-8">
    
      <div class="title">Rectified Factor Networks for Biclustering of Omics Data</div>
      <div class="author">
        
          
            
              
                
                  Clevert, D.,
                
              
            
          
        
          
            
              
                
                  Unterthiner, T.,
                
              
            
          
        
          
            
              
                
                  Povysil, G.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1093/bioinformatics/btx226" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Biclustering has become a major tool for analyzing large datasets given as matrix of samples times features and has been successfully applied in life sciences and e-commerce for drug design and recommender systems, respectively. Factor Analysis for Bicluster Acquisition (FABIA), one of the most successful biclustering methods, is a generative model that represents each bicluster by two sparse membership vectors: one for the samples and one for the features. However, FABIA is restricted to about 20 code units because of the high computational complexity of computing the posterior. Furthermore, code units are sometimes insufficiently decorrelated and sample membership is difficult to determine. We propose to use the recently introduced unsupervised Deep Learning approach Rectified Factor Networks (RFNs) to overcome the drawbacks of existing biclustering methods. RFNs efficiently construct very sparse, non-linear, high-dimensional representations of the input via their posterior means. RFN learning is a generalized alternating minimization algorithm based on the posterior regularization method which enforces non-negative and normalized posterior means. Each code unit represents a bicluster, where samples for which the code unit is active belong to the bicluster and features that have activating weights to the code unit belong to the bicluster.On 400 benchmark datasets and on three gene expression datasets with known clusters, RFN outperformed 13 other biclustering methods including FABIA. On data of the 1000 Genomes Project, RFN could identify DNA segments which indicate, that interbreeding with other hominins starting already before ancestors of modern humans left Africa.https://github.com/bioinf-jku/librfn</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="greiffLearningHighDimensionalImmunogenomic2017" class="col-sm-8">
    
      <div class="title">Learning the High-Dimensional Immunogenomic Features That Predict Public and Private Antibody Repertoires</div>
      <div class="author">
        
          
            
              
                
                  Greiff, V.,
                
              
            
          
        
          
            
              
                
                  Weber, C.,
                
              
            
          
        
          
            
              
                
                  Palme, J.,
                
              
            
          
        
          
            
              
                
                  Bodenhofer, U.,
                
              
            
          
        
          
            
              
                
                  Miho, E.,
                
              
            
          
        
          
            
              
                
                  Menzel, U.,
                
              
            
          
        
          
            
              
                
                  and Reddy, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.4049/jimmunol.1700594" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent studies have revealed that immune repertoires contain a substantial fraction of public clones, which may be defined as Ab or TCR clonal sequences shared across individuals. It has remained unclear whether public clones possess predictable sequence features that differentiate them from private clones, which are believed to be generated largely stochastically. This knowledge gap represents a lack of insight into the shaping of immune repertoire diversity. Leveraging a machine learning approach capable of capturing the high-dimensional compositional information of each clonal sequence (defined by CDR3), we detected predictive public clone and private clone–specific immunogenomic differences concentrated in CDR3’s N1–D–N2 region, which allowed the prediction of public and private status with 80% accuracy in humans and mice. Our results unexpectedly demonstrate that public, as well as private, clones possess predictable high-dimensional immunogenomic features. Our support vector machine model could be trained effectively on large published datasets (3 million clonal sequences) and was sufficiently robust for public clone prediction across individuals and studies prepared with different library preparation and high-throughput sequencing protocols. In summary, we have uncovered the existence of high-dimensional immunogenomic rules that shape immune repertoire diversity in a predictable fashion. Our approach may pave the way for the construction of a comprehensive atlas of public mouse and human immune repertoires with potential applications in rational vaccine design and immunotherapeutics.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="heuselGANsTrainedTwo2017" class="col-sm-8">
    
      <div class="title">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</div>
      <div class="author">
        
          
            
              
                
                  Heusel, M.,
                
              
            
          
        
          
            
              
                
                  Ramsauer, H.,
                
              
            
          
        
          
            
              
                
                  Unterthiner, T.,
                
              
            
          
        
          
            
              
                
                  Nessler, B.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="http://arxiv.org/abs/1706.08500" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/1706.08500.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\’echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div>

  <div id="klambauerSelfNormalizingNeuralNetworks2017" class="col-sm-8">
    
      <div class="title">Self-Normalizing Neural Networks</div>
      <div class="author">
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  Unterthiner, T.,
                
              
            
          
        
          
            
              
                
                  Mayr, A.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="http://arxiv.org/abs/1706.02515" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/1706.02515.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance – even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="povysilPanelcnMOPSCopynumber2017" class="col-sm-8">
    
      <div class="title">Panelcn.MOPS: Copy-Number Detection in Targeted NGS Panel Data for Clinical Diagnostics</div>
      <div class="author">
        
          
            
              
                
                  Povysil, G.,
                
              
            
          
        
          
            
              
                
                  Tzika, A.,
                
              
            
          
        
          
            
              
                
                  Vogt, J.,
                
              
            
          
        
          
            
              
                
                  Haunschmid, V.,
                
              
            
          
        
          
            
              
                
                  Messiaen, L.,
                
              
            
          
        
          
            
              
                
                  Zschocke, J.,
                
              
            
          
        
          
            
              
                
                  Klambauer, G.,
                
              
            
          
        
          
            
              
                
                  Hochreiter, S.,
                
              
            
          
        
          
            
              
                
                  and Wimmer, K.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.1002/humu.23237" class="btn btn-sm z-depth-0" role="button" target="_blank">url</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Targeted next-generation-sequencing (NGS) panels have largely replaced Sanger sequencing in clinical diagnostics. They allow for the detection of copy-number variations (CNVs) in addition to single-nucleotide variants and small insertions/deletions. However, existing computational CNV detection methods have shortcomings regarding accuracy, quality control (QC), incidental findings, and user-friendliness. We developed panelcn.MOPS, a novel pipeline for detecting CNVs in targeted NGS panel data. Using data from 180 samples, we compared panelcn.MOPS with five state-of-the-art methods. With panelcn.MOPS leading the field, most methods achieved comparably high accuracy. panelcn.MOPS reliably detected CNVs ranging in size from part of a region of interest (ROI), to whole genes, which may comprise all ROIs investigated in a given sample. The latter is enabled by analyzing reads from all ROIs of the panel, but presenting results exclusively for user-selected genes, thus avoiding incidental findings. Additionally, panelcn.MOPS offers QC criteria not only for samples, but also for individual ROIs within a sample, which increases the confidence in called CNVs. panelcn.MOPS is freely available both as R package and standalone software with graphical user interface that is easy to use for clinical geneticists without any programming experience. panelcn.MOPS combines high sensitivity and specificity with user-friendliness rendering it highly suitable for routine clinical diagnostics.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div>

  <div id="treml2016" class="col-sm-8">
    
      <div class="title">Speeding up Semantic Segmentation for Autonomous Driving</div>
      <div class="author">
        
          
            
              
                
                  Treml, M.,
                
              
            
          
        
          
            
              
                
                  Arjona-Medina, J.,
                
              
            
          
        
          
            
              
                
                  Unterthiner, T.,
                
              
            
          
        
          
            
              
                
                  Durgesh, R.,
                
              
            
          
        
          
            
              
                
                  Friedmann, F.,
                
              
            
          
        
          
            
              
                
                  Schuberth, P.,
                
              
            
          
        
          
            
              
                
                  Mayr, A.,
                
              
            
          
        
          
            
              
                
                  Heusel, M.,
                
              
            
          
        
          
            
              
                
                  Hofmarcher, M.,
                
              
            
          
        
          
            
              
                
                  Widrich, M.,
                
              
            
          
        
          
            
              
                
                  Bodenhofer, U.,
                
              
            
          
        
          
            
              
                
                  Nessler, B.,
                
              
            
          
        
          
            
              
                
                  and Hochreiter, S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Workshop on Machine Learning for Intelligent Transport Systems, Neural Information Processing Systems (NIPS)</em>
      
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"></ol>

  <h2 class="year">2014</h2>
  <ol class="bibliography"></ol>

  <h2 class="year">2013</h2>
  <ol class="bibliography"></ol>

  <h2 class="year">2012</h2>
  <ol class="bibliography"></ol>


</div>

  </div>
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2026 Institute for Machine Learning @ JKU.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
